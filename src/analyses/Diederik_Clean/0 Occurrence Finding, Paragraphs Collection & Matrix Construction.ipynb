{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfff6ea5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; border-radius: 5px; padding: 10px;\">\n",
    "    <h4>Occurence Finding, Paragraphs Collection and Matrix Construction</h4>\n",
    "    <p>...</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9c10f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a502f7",
   "metadata": {},
   "source": [
    "## Packages and Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aec1308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unidecode\n",
    "\n",
    "\n",
    "# Import Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9372f22d",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625aeacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read all files from a directory\n",
    "def read_stream(indir):\n",
    "    \"\"\"\n",
    "    Function to read all the files in a directory (or nested directories)\n",
    "    containing wikidump extracts.\n",
    "    Returns a list of strings each element representing the entire contents\n",
    "    of a single file\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        indir (str): path to a directory containing text files\n",
    "                     or directories with text files\n",
    "\n",
    "    \"\"\"\n",
    "    wikidump = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for root, dirs, files in os.walk(indir):\n",
    "\n",
    "        for filename in files:\n",
    "            if not filename.startswith(\".\"):\n",
    "                fp = os.path.join(root, filename)\n",
    "\n",
    "                with open(fp, 'r') as f:\n",
    "                    wikidump.append(f.read())\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    total = t1-t0\n",
    "    print(f\"It took {total}s to read {indir}.\")\n",
    "\n",
    "    return wikidump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35e71de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split dumps into flat list\n",
    "def split_dump(input_dump, split_pattern = \"c>\"):\n",
    "    \"\"\"\n",
    "    splits list of wikidump documents into a flat list of articles\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            input_dump:    a list of strings\n",
    "            split_pattern: str, optional\n",
    "                string pattern at which the strings\n",
    "                should be split into articles. default = 'c>'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    article_list = [\n",
    "        article for dump\n",
    "        in tqdm(input_dump, total = len(input_dump), desc = \"Progress split_dump()\")\n",
    "        for article in dump.split(split_pattern)]\n",
    "\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c267535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dump2(dump, key_words, message = True):\n",
    "    \"\"\"extracts titles and ids from articles containing key words and returns as a list\"\"\"\n",
    "\n",
    "    articles = []\n",
    "    for article in tqdm(dump, total = len(dump), desc = \"Progress process_dump()\"):\n",
    "        article = unidecode.unidecode(article)\n",
    "        if (list_in_corpus(key_words, article)):\n",
    "            try:\n",
    "                article_id = find_id(article)\n",
    "                title = find_title(article)\n",
    "                article_body = find_article(article)\n",
    "                articles.append((article_id, title, article_body))\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if message:\n",
    "        print(f\"{len(articles)} articles out of {len(dump)} contain 2 toponyms\")\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ef4c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function integrating the other functions\n",
    "def preprocess(base_dir, outdir, key_words, remove_referral=True, overwrite_protection=True):\n",
    "    \"\"\"\n",
    "        params:\n",
    "            base_dir:             str;\n",
    "                path to directory where extracted wikidump files can be found\n",
    "            outdir:               str;\n",
    "                path where processed files will be saved to (one file per multistream)\n",
    "            language:             str;\n",
    "                one of the following ['en', 'fr']\n",
    "            key_words:            str, list;\n",
    "                list of strings which must be included in article\n",
    "            remove_referral:      bool, optional; default is True.\n",
    "                if True referral pages will be removed\n",
    "            overwrite_protection: bool, optional; default is True.\n",
    "                if True confirmation will be asked before overwriting files\n",
    "    \"\"\"\n",
    "\n",
    "    # establish that a valid language was chosen, if not abort function:\n",
    "#     lang_list = ['fr', 'en']\n",
    "#     if language not in lang_list:\n",
    "#         print(f\"Invalid language was chosen. \\n Please choose one of the following: {lang_list}\")\n",
    "#         return\n",
    "\n",
    "    # creating an output directory\n",
    "    outdir = os.path.join(outdir, 'enwiki/')\n",
    "\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "        print(f'created directory at: {outdir}')\n",
    "\n",
    "\n",
    "    # list of multistream directories in base_dir\n",
    "    dir_list = os.listdir(base_dir)\n",
    "\n",
    "\n",
    "#     for directory in dir_list:\n",
    "    for directory in tqdm(dir_list, total = len(dir_list), desc = \"Progress Total\"):\n",
    "        dir_fp = os.path.join(base_dir, directory)\n",
    "        \n",
    "        if not directory.startswith(\".\"):\n",
    "            print(f\"\\nStarting preprocessing on: {dir_fp}\")\n",
    "            wikidump = read_stream(dir_fp) # read the files in the directory\n",
    "\n",
    "            wikidump = split_dump(wikidump) # split the files\n",
    "            wikidump = process_dump2(wikidump, key_words) # extract id, title, article\n",
    "\n",
    "            df = pd.DataFrame(wikidump, columns = ['article_id', 'title', 'text'])\n",
    "\n",
    "            if remove_referral:\n",
    "                try:\n",
    "                    df['length'] = [len(text.split()) for text in df.text]\n",
    "                    df['length_title'] = [len(title.split()) for title in df.title]\n",
    "                    n_referral = len(df[df.length == df.length_title])\n",
    "\n",
    "                    df = df[['article_id', 'title', 'text']][df.length != df.length_title]\n",
    "                    print(f\"Removing {n_referral} referral pages\")\n",
    "\n",
    "                except:\n",
    "                    print(f\"Referral pages were not removed from multistream {directory}\")\n",
    "                    pass\n",
    "\n",
    "\n",
    "            # saving the output\n",
    "            outfile = f'enwikidump_{directory}.csv'\n",
    "            outputfp = os.path.join(outdir, outfile)\n",
    "\n",
    "            # call write_outputcsv function\n",
    "            write_outputcsv(df, outputfp, overwrite_protection = overwrite_protection)\n",
    "        else:\n",
    "            print(f\"Skipping: {dir_fp}\")\n",
    "\n",
    "    print(f\"----------\\nFiles in {base_dir} have been processed\\n----------\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82bbc044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix generation related functions\n",
    "def create_city_dict(city_list):\n",
    "    \"\"\"\n",
    "    function that creates a dictionary of name variants to the standard form\n",
    "    output: a dictionary where the keys are variant names and the values are\n",
    "    standard names.\n",
    "    \"\"\"\n",
    "\n",
    "    # instantiate dictionary\n",
    "    city_dict = dict()\n",
    "\n",
    "    # split up the city names in the city list where a '-' occurs\n",
    "    # (the symbol used to split separate placenames)\n",
    "    for city in city_list:\n",
    "        keys = city.split('-')\n",
    "        if len(keys) > 1:\n",
    "            keys.append(city)\n",
    "        for key in keys:\n",
    "            city_dict[key] = city\n",
    "\n",
    "    return city_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54113de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_matrix(city_list):\n",
    "    \"\"\"generates an empty matrix with the index/columns consisting of the city names\"\"\"\n",
    "\n",
    "    # create zero matrix with the correct dimensions\n",
    "    matrix = np.zeros((len(city_list), len(city_list)))\n",
    "\n",
    "    # transform into dataframe with the columns and index set to the list of cities\n",
    "    matrix = pd.DataFrame(matrix, columns = city_list)\n",
    "    matrix['index'] = city_list\n",
    "    matrix.set_index('index', inplace = True)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae9b9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_appearance(text, dictionary):\n",
    "    \"\"\"function to check which placenames appear in the input text per paragraph\"\"\"\n",
    "\n",
    "    # instantiate empty list of standardised city names and city name variations\n",
    "    cities_variants = []\n",
    "    cities_standard = []\n",
    "\n",
    "    # for each word in the text check if the word is a key word in the dictionary(one of the variants)\n",
    "    for word in dictionary:\n",
    "        pattern = r\"\\b\" + word + r\"\\b\" #add word boundaries to dictionary word\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            cities_variants.append(word)\n",
    "\n",
    "    # for each word in the variant replace name with the standard form\n",
    "    for city in cities_variants:\n",
    "        city_standard = city.replace(city, dictionary[city])\n",
    "        cities_standard.append(city_standard)\n",
    "\n",
    "    return cities_variants, cities_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b13ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(article, dictionary, matrix):\n",
    "    \"\"\"IMPROVE DOC string\n",
    "    function that processes each article in order to update the co-occurence values\n",
    "    in a co-occurence matrix\"\"\"\n",
    "\n",
    "    # split article into paragraphs (by using '\\n' as end of paragraph)\n",
    "    paragraphs = article.splitlines()\n",
    "    for paragraph in paragraphs:\n",
    "        \n",
    "        # if paragraph empty skip\n",
    "        if not paragraph:\n",
    "            continue\n",
    "\n",
    "        # generate list of cities that appear in the paragraph\n",
    "        cities_variants, cities_standard = city_appearance(paragraph, dictionary)\n",
    "\n",
    "        # skip if fewer than 2 cities appear\n",
    "        if len(set(cities_standard)) < 2:\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            # create the co-occurences that appear\n",
    "            for city_i in cities_standard:\n",
    "                for city_j in cities_standard:\n",
    "                    if city_i != city_j: # make sure cities don't co-occure with themselves\n",
    "                        matrix.at[city_i, city_j] += 1 # update value in matrix\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "18cc29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(corpus, city_list):\n",
    "    \"\"\"function that processes the entire corpus and creates co-occurence matrix\"\"\"\n",
    "\n",
    "    # generate dictionary and matrix and paragraphs dataframe\n",
    "    dictionary = create_city_dict(city_list)\n",
    "    matrix = city_matrix(city_list)\n",
    "    \n",
    "    # loop over each article in the corpus and update the matrix\n",
    "    for article in tqdm(corpus, total = len(corpus), desc = \"Articles processed\"):\n",
    "        process_article(article, dictionary, matrix)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "636a1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_matrix(matrix, outdir, filename):\n",
    "    \"\"\"function to write matrix to csv\"\"\"\n",
    "    outfp = os.path.join(outdir, filename)\n",
    "\n",
    "    if os.path.exists(outfp):\n",
    "        print(f\"File {outfp} already exists.\")\n",
    "        print(\"Are you sure you want to continue and overwrite the file?\")\n",
    "        decision = input('Continue? [y/n]')\n",
    "        if decision == 'y':\n",
    "            matrix.to_csv(outfp, index = True)\n",
    "            print(f\"Matrix has been written to: {outfp}\")\n",
    "        elif decision == 'n':\n",
    "            print(\"The process has been halted.\")\n",
    "        else:\n",
    "            print(\"You did not enter a valid option.\\nThe process has halted.\")\n",
    "    else:\n",
    "        matrix.to_csv(outfp, index = True)\n",
    "        print(f\"Matrix has been written to: {outfp}\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd68d8",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ef58604",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '../../input/List_of_cities_300k.csv' # path to csv with city information\n",
    "cities = pd.read_csv(fp, sep=';')\n",
    "\n",
    "name_col = f'Mua_en'\n",
    "cities_list = [unidecode.unidecode(city_component) \n",
    "             for city in cities[name_col] \n",
    "             for city_component in city.split('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a7b8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../../data_clean/' # directory where selected articles will be saved, change if you want to save these elsewhere\n",
    "out_dir = 'output/'\n",
    "in_dir = '../../input/'\n",
    "# extr_dir = path/to/wikidump/extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea6c802",
   "metadata": {},
   "source": [
    "## Discard Unrelated Articles\n",
    "This creates multiple .csv files with the 'title', 'title', 'text' of wikipedia articles with at least 2 toponym ocurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dir = f\"enwiki_extracted\" # language specific directory\n",
    "base_dir = os.path.join(extr_dir, wiki_dir)\n",
    "\n",
    "# do the whole preprocessing/extraction thing\n",
    "preprocess(base_dir, data_dir, lang, cities_list, overwrite_protection = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c0449",
   "metadata": {},
   "source": [
    "## Create Co-Occurrence Matrix\n",
    "Iterates over the .csv files created in the previous chunk to identify toponym co-occurences and add these to a matrix. Function documentation within `preprocessing_functions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32774b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of complete city names\n",
    "city_l = [unidecode.unidecode(city) for city in cities[name_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47746d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8e21f3d62c437385712f5fec2be252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Articles processed:   0%|          | 0/508671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_id\n",
      "title\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "# # create matrix\n",
    "# matrix = process_corpus(df, city_l)\n",
    "\n",
    "# # save matrix\n",
    "# FILENAME = f\"en_matrix.csv\"\n",
    "\n",
    "# #write_matrix(matrix = matrix, \n",
    "# #             outdir = out_dir, \n",
    "# #             filename = FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793212e",
   "metadata": {},
   "source": [
    "## Articles with city pair co-occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "40e72b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30143</td>\n",
       "      <td>Economy of Togo</td>\n",
       "      <td>\\nEconomy of Togo\\n\\nThe economy of Togo has s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30159</td>\n",
       "      <td>History of Tonga</td>\n",
       "      <td>\\nHistory of Tonga\\n\\nThe history of Tonga is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30169</td>\n",
       "      <td>History of Trinidad and Tobago</td>\n",
       "      <td>\\nHistory of Trinidad and Tobago\\n\\nThe histor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30178</td>\n",
       "      <td>Tromelin Island</td>\n",
       "      <td>\\nTromelin Island\\n\\nTromelin Island (; , ) is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30212</td>\n",
       "      <td>Economy of Turkmenistan</td>\n",
       "      <td>\\nEconomy of Turkmenistan\\n\\nThe economy of Tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17413</th>\n",
       "      <td>3963512</td>\n",
       "      <td>Elchonon Wasserman</td>\n",
       "      <td>\\nElchonon Wasserman\\n\\nElchonon Bunim Wasserm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17414</th>\n",
       "      <td>3963783</td>\n",
       "      <td>Naipes Heraclio Fournier</td>\n",
       "      <td>\\nNaipes Heraclio Fournier\\n\\nNaipes Heraclio ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17415</th>\n",
       "      <td>3963794</td>\n",
       "      <td>Banovina of Croatia</td>\n",
       "      <td>\\nBanovina of Croatia\\n\\nThe Banovina of Croat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17416</th>\n",
       "      <td>3963903</td>\n",
       "      <td>II Corps (Australia)</td>\n",
       "      <td>\\nII Corps (Australia)\\n\\nII Corps was an Aust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17417</th>\n",
       "      <td>3963926</td>\n",
       "      <td>Picton railway station</td>\n",
       "      <td>\\nPicton railway station\\n\\nPicton railway sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>508671 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id                           title  \\\n",
       "0          30143                 Economy of Togo   \n",
       "1          30159                History of Tonga   \n",
       "2          30169  History of Trinidad and Tobago   \n",
       "3          30178                 Tromelin Island   \n",
       "4          30212         Economy of Turkmenistan   \n",
       "...          ...                             ...   \n",
       "17413    3963512              Elchonon Wasserman   \n",
       "17414    3963783        Naipes Heraclio Fournier   \n",
       "17415    3963794             Banovina of Croatia   \n",
       "17416    3963903            II Corps (Australia)   \n",
       "17417    3963926          Picton railway station   \n",
       "\n",
       "                                                    text  \n",
       "0      \\nEconomy of Togo\\n\\nThe economy of Togo has s...  \n",
       "1      \\nHistory of Tonga\\n\\nThe history of Tonga is ...  \n",
       "2      \\nHistory of Trinidad and Tobago\\n\\nThe histor...  \n",
       "3      \\nTromelin Island\\n\\nTromelin Island (; , ) is...  \n",
       "4      \\nEconomy of Turkmenistan\\n\\nThe economy of Tu...  \n",
       "...                                                  ...  \n",
       "17413  \\nElchonon Wasserman\\n\\nElchonon Bunim Wasserm...  \n",
       "17414  \\nNaipes Heraclio Fournier\\n\\nNaipes Heraclio ...  \n",
       "17415  \\nBanovina of Croatia\\n\\nThe Banovina of Croat...  \n",
       "17416  \\nII Corps (Australia)\\n\\nII Corps was an Aust...  \n",
       "17417  \\nPicton railway station\\n\\nPicton railway sta...  \n",
       "\n",
       "[508671 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# folder of extracted streams\n",
    "inputfp = os.path.join(data_dir, f'enwiki/')\n",
    "\n",
    "#- loop over .csv files create dataframes\n",
    "df = pd.DataFrame(columns = ['article_id', 'title', 'text'])\n",
    "\n",
    "# iterate over directory for each file path, create a dataframe\n",
    "for file in os.listdir(inputfp):\n",
    "    fp = os.path.join(inputfp, file)\n",
    "    df_temp = pd.read_csv(fp)\n",
    "    df = pd.concat([df, df_temp])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8604ec58",
   "metadata": {},
   "source": [
    "Turning dataframe into a dictionary to improve computational speed (see speed tests at the bottom of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1f23f07b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.33 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "df_dict = df.to_dict('records')\n",
    "sorted_df_dict = sorted(df_dict, key=lambda d: d['article_id']) \n",
    "# paragraphs_df = pd.DataFrame(columns= ['city_pair', 'paragraph_id', 'paragraph', 'article_id', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d8a41",
   "metadata": {},
   "source": [
    "### Chunk up articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "fedcdb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num, div = len(sorted_df_dict), 5\n",
    "chunks = [num // div + (1 if x < num % div else 0)  for x in range (div)]\n",
    "cum_chunks = [0]\n",
    "\n",
    "for i, x in enumerate(chunks):\n",
    "    cum_chunks.append(sum(chunks[:i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "6f2f0386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 101735),\n",
       " (101735, 203469),\n",
       " (203469, 305203),\n",
       " (305203, 406937),\n",
       " (406937, 508671)]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_min_max = list(zip(cum_chunks, cum_chunks[1:]))\n",
    "chunks_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "babcbd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1659534902'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get date/time of code running, could be useful\n",
    "# str(time.time()).split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284bba89",
   "metadata": {},
   "source": [
    "## Collecting paragraphs with city pair co-occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "2b6e5468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7cf8011f6924bbb9584e65f52137da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85481ecc82464914be70b8f205ca6e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 - 101735:   0%|          | 0/101735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b576148c154047e8b9ab5b3ff1fd27be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "101735 - 203469:   0%|          | 0/101734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6932ebd1917540bebb6739eb6c0a9506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "203469 - 305203:   0%|          | 0/101734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f75d578b0654d9f81ee00c8a3203af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "305203 - 406937:   0%|          | 0/101734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647adc5e70354bb4a5bd13bde0fdf60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "406937 - 508671:   0%|          | 0/101734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2h 55min 2s\n",
      "Wall time: 2h 57min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dictionary = create_city_dict(city_l)\n",
    "paragraphs_df = pd.DataFrame(columns= ['city_1', 'city_2', 'paragraph_id', 'paragraph', 'article_id', 'title'])\n",
    "\n",
    "count = 0\n",
    "for chunk in tqdm(chunks_min_max):\n",
    "    list_of_paragraphs=[]\n",
    "    file_path = f\"paragraphs_{chunk[0]}_{chunk[1]}.csv\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"{file_path} already exists.\")\n",
    "        continue\n",
    "        \n",
    "    for row in tqdm(sorted_df_dict[chunk[0]:chunk[1]], desc = f\"{chunk[0]} - {chunk[1]}\"):\n",
    "        # split article into paragraphs (by using '\\n' as end of paragraph)\n",
    "        paragraphs = row['text'].splitlines()\n",
    "        for paragraph in paragraphs:\n",
    "\n",
    "            # if paragraph empty skip\n",
    "            if not paragraph:\n",
    "                continue\n",
    "\n",
    "            # generate list of cities that appear in the paragraph\n",
    "            cities_variants, cities_standard = city_appearance(paragraph, dictionary)\n",
    "\n",
    "            # skip if fewer than 2 cities appear\n",
    "            if len(set(cities_standard)) < 2:\n",
    "                continue\n",
    "\n",
    "            for city_i in cities_standard:\n",
    "                for city_j in cities_standard:\n",
    "                    if city_i != city_j:\n",
    "                        count += 1\n",
    "                        list_of_paragraphs.append([city_i, city_j, count, paragraph, row['article_id'], row['title']])\n",
    "                    \n",
    "    temp_paragraphs_df = pd.DataFrame(columns= ['city_1', 'city_2', 'paragraph_id', 'paragraph', 'article_id', 'title'], data = list_of_paragraphs)\n",
    "    temp_paragraphs_df.to_csv(f\"paragraphs_{chunk[0]}_{chunk[1]}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff31f33",
   "metadata": {},
   "source": [
    "## Merge all paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "56fd1d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paragraphs_df = pd.DataFrame(columns= ['city_1', 'city_2', 'paragraph_id', 'paragraph', 'article_id', 'title'])\n",
    "\n",
    "for chunk in chunks_min_max:\n",
    "    temp_df2 = pd.read_csv(os.path.join(data_dir, \"paragraphs\", f\"paragraphs_{chunk[0]}_{chunk[1]}.csv\"))\n",
    "    all_paragraphs_df = pd.concat([paragraphs_df, temp_df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "fc3711a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2076404"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_paragraphs_df['paragraph'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06565b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_scanned_article = temp_paragraphs_df.iloc[-1].article_id\n",
    "# len(list_of_paragraphs)\n",
    "subset_paragraphs_df = pd.read_csv(os.path.join(data_dir, \"paragraphs\", f\"paragraphs_0_101735.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b1227f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_1</th>\n",
       "      <th>city_2</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Birmingham</td>\n",
       "      <td>Florence</td>\n",
       "      <td>1</td>\n",
       "      <td>The first community of adherents of the Baha'i...</td>\n",
       "      <td>303</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Florence</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>2</td>\n",
       "      <td>The first community of adherents of the Baha'i...</td>\n",
       "      <td>303</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paris</td>\n",
       "      <td>London</td>\n",
       "      <td>3</td>\n",
       "      <td>A major revision of the work by composer and a...</td>\n",
       "      <td>309</td>\n",
       "      <td>An American in Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>London</td>\n",
       "      <td>Paris</td>\n",
       "      <td>4</td>\n",
       "      <td>A major revision of the work by composer and a...</td>\n",
       "      <td>309</td>\n",
       "      <td>An American in Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Madrid</td>\n",
       "      <td>Rome</td>\n",
       "      <td>5</td>\n",
       "      <td>Access to biocapacity in Algeria is lower than...</td>\n",
       "      <td>358</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488331</th>\n",
       "      <td>Dublin</td>\n",
       "      <td>London</td>\n",
       "      <td>488332</td>\n",
       "      <td>Allman-Smith played hockey for Dublin Universi...</td>\n",
       "      <td>3001932</td>\n",
       "      <td>Edward Allman-Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488332</th>\n",
       "      <td>London</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>488333</td>\n",
       "      <td>O'Kelly and Condell met in Dublin in 1969 and ...</td>\n",
       "      <td>3001953</td>\n",
       "      <td>Tir na nOg (band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488333</th>\n",
       "      <td>Dublin</td>\n",
       "      <td>London</td>\n",
       "      <td>488334</td>\n",
       "      <td>O'Kelly and Condell met in Dublin in 1969 and ...</td>\n",
       "      <td>3001953</td>\n",
       "      <td>Tir na nOg (band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488334</th>\n",
       "      <td>Birmingham</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>488335</td>\n",
       "      <td>Tir na nOg reformed in 1985, releasing the sin...</td>\n",
       "      <td>3001953</td>\n",
       "      <td>Tir na nOg (band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488335</th>\n",
       "      <td>Dublin</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>488336</td>\n",
       "      <td>Tir na nOg reformed in 1985, releasing the sin...</td>\n",
       "      <td>3001953</td>\n",
       "      <td>Tir na nOg (band)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>488336 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            city_1      city_2  paragraph_id  \\\n",
       "0       Birmingham    Florence             1   \n",
       "1         Florence  Birmingham             2   \n",
       "2            Paris      London             3   \n",
       "3           London       Paris             4   \n",
       "4           Madrid        Rome             5   \n",
       "...            ...         ...           ...   \n",
       "488331      Dublin      London        488332   \n",
       "488332      London      Dublin        488333   \n",
       "488333      Dublin      London        488334   \n",
       "488334  Birmingham      Dublin        488335   \n",
       "488335      Dublin  Birmingham        488336   \n",
       "\n",
       "                                                paragraph  article_id  \\\n",
       "0       The first community of adherents of the Baha'i...         303   \n",
       "1       The first community of adherents of the Baha'i...         303   \n",
       "2       A major revision of the work by composer and a...         309   \n",
       "3       A major revision of the work by composer and a...         309   \n",
       "4       Access to biocapacity in Algeria is lower than...         358   \n",
       "...                                                   ...         ...   \n",
       "488331  Allman-Smith played hockey for Dublin Universi...     3001932   \n",
       "488332  O'Kelly and Condell met in Dublin in 1969 and ...     3001953   \n",
       "488333  O'Kelly and Condell met in Dublin in 1969 and ...     3001953   \n",
       "488334  Tir na nOg reformed in 1985, releasing the sin...     3001953   \n",
       "488335  Tir na nOg reformed in 1985, releasing the sin...     3001953   \n",
       "\n",
       "                       title  \n",
       "0                    Alabama  \n",
       "1                    Alabama  \n",
       "2       An American in Paris  \n",
       "3       An American in Paris  \n",
       "4                    Algeria  \n",
       "...                      ...  \n",
       "488331   Edward Allman-Smith  \n",
       "488332     Tir na nOg (band)  \n",
       "488333     Tir na nOg (band)  \n",
       "488334     Tir na nOg (band)  \n",
       "488335     Tir na nOg (band)  \n",
       "\n",
       "[488336 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_paragraphs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d16a212",
   "metadata": {},
   "source": [
    "## Extra: Speed Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8b85ba59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3c622cb7d842d99570df8a1c680b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/508671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 188 ms\n",
      "Wall time: 166 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# dictionary iteration\n",
    "for row in tqdm(df_dict):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c55122d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4cc3d0296a4045a576dfa71accbbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/508671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 250 ms\n",
      "Wall time: 201 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# numpy array iteration\n",
    "for row in tqdm(df.values):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5f8a3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad042fa24424337b385b4b66d7b6336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 453 ms\n",
      "Wall time: 494 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# itertuples\n",
    "for row in tqdm(df.itertuples()):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f72a1f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20e25407acd4c8db33319282212f6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 12.2 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# iterrows\n",
    "for row in tqdm(df.iterrows()):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
