{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c643808",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; border-radius: 5px; padding: 10px;\">\n",
    "    <h4>Word Embedding Categorisation</h4>\n",
    "    <p>...</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_functions import *\n",
    "from word_embedding_functions import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e356c",
   "metadata": {},
   "source": [
    "- Clean and preprocess (lemmatise) a list of documents (e.g. paragraphs)\n",
    "- Get your topics through unsupervised clustering with (LDA Topic Modeling)\n",
    "- Use these for the word embedding algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127441f",
   "metadata": {},
   "source": [
    "### 1. Set Topic Words\n",
    "<br>\n",
    "<div style=\"text-align: justify;\">Using LDA topic modeling to find topics and their words through unsupervised clustering highly increases the performance of the model. Look at the pyLDAvis visualisation of the LDA topic model and change any parameters if you are not happy with the cluster distributions or if any clusters are overlapping. </div>\n",
    "<br>\n",
    "<div style=\"text-align: justify;\">\n",
    "A good topic model will have relatively big, similarly sized and non-overlapping bubbles scattered throughout the chart. Greater distances between the clusters represents a larger semantic difference, similarly sized bubbles show that the topics are equally represented, and large circles mean that the topics are well represented in the documents. By paying attention to these three characteristics we can get an accurate representation of the dominant topics of our documents and decide whether these clusters represent good, meaningful topics.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422727f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing words with double meaning helps (e.g. bear, degree)\n",
    "\n",
    "#fashion_words = ['fashion', 'model', 'vogue', 'store', 'designer', 'couture', 'catwalk', 'runway', 'modeling', 'clothing', 'cosmetic', 'brand', 'retail', 'advertising', 'perfume']\n",
    "diplomacy_words = ['War', 'Embassy', 'Army', 'Diplomatic', 'Ambassador', 'Treaty', 'Protest', 'Force', 'Mission', 'Arrest', 'Government', 'Police', 'Attack', 'Party', 'Minister']\n",
    "entertainment_words = ['Opera', 'Festival', 'Perform', 'Orchestra', 'Symphony', 'Concert', 'Music', 'Film', 'Sing', 'Theatre', 'Performance', 'Role', 'Premiere', 'Tour', 'Band']\n",
    "art_words = ['Exhibition', 'Art', 'Museum', 'Gallery', 'Exhibit', 'Painting', 'Collection', 'Paint', 'Portrait', 'Artist', 'Sculpture', 'Fashion', 'Design', 'Contemporary', 'Painter']\n",
    "education_words = ['Study', 'School', 'Professor', 'University', 'Graduate', 'Educate', 'Lecture', 'Research', 'College', 'Teach', 'Science', 'Education', 'Philosophy', 'Doctorate', 'Faculty'] # replaced bear with educate and degree with Lecture\n",
    "transportation_words = ['Railway', 'Route', 'Line', 'Operate', 'Flight', 'Station', 'Service', 'Airline', 'Airport', 'Train', 'Passenger', 'Speed', 'Aircraft', 'Rail', 'Network']\n",
    "sport_words = ['Final', 'Win', 'Team', 'Match', 'Game', 'Goal', 'Club', 'League', 'Champion', 'Championship', 'Season', 'Score', 'Round', 'Tournament', 'Football']\n",
    "\n",
    "lda_topic_words = { #'fashion': fashion_words,\n",
    "                    'diplomacy': diplomacy_words,\n",
    "                   'entertainment': entertainment_words,\n",
    "                   'art': art_words,\n",
    "                   'education': education_words,\n",
    "                   'transportation': transportation_words,\n",
    "                   'sport': sport_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2195b8",
   "metadata": {},
   "source": [
    "### 2. Loading GloVe Word Embedding\n",
    "<br>\n",
    "<div style=\"text-align: justify;\"> The word embedding model could be replaced by other ones fairly easy, but for now we decided to use the Common Crawl 840B token one  (source: https://nlp.stanford.edu/projects/glove/).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c59a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings_dict, discarded_dict = load_glove_word_embeddings(GLOVE_PATH=\"../../../../../glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63be62",
   "metadata": {},
   "source": [
    "### 3. Get Mean of the words from each Topic\n",
    "<br>\n",
    "<div style=\"text-align: justify;\">By taking the mean of the vectors that belong to the ~15 most relevant words of a topic we get a fairly accurate vector representation of a topic. We do this for each topic and save their vector value to a dictionary for later retrieval.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e65d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vectors_dict = {}\n",
    "for topic in lda_topic_words:\n",
    "    words = lda_topic_words[topic]\n",
    "    words = [word for word in words if word in embeddings_dict.keys()] # checks if word is in vocabulary (i.e. has been seen by the model before)\n",
    "    mean_embedding = np.mean([embeddings_dict[word.lower()] for word in words], axis=0)\n",
    "    mean_vectors_dict[topic] = mean_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98718a8d",
   "metadata": {},
   "source": [
    "### 4. Load Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea14cd4",
   "metadata": {},
   "source": [
    "#### 4.1 Load and merge chunked .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d189714",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "# document_path = #\n",
    "# df =  pd.read_csv(document_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861ea5c",
   "metadata": {},
   "source": [
    "#### 4.2 Select number of paragraphs to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c188296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234aa540",
   "metadata": {},
   "source": [
    "#### 4.3 Turn stringed lists (in csv) into list objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2212c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "df['merged_POS'] = df['merged_POS'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82403b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6275204d",
   "metadata": {},
   "source": [
    "### 5. Classify Paragraphs (by Word Embedding Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9c479",
   "metadata": {},
   "source": [
    "#### 5.1 Select right parameters\n",
    "<br>\n",
    "<div style=\"text-align: justify;\">bottom_threshold: If the similarity between a word and its closest topic is below the bottom_threshold it will be discarded from the classification process.\n",
    "Verbose1 and Verbose2: Enabling these will print out the internal process of the algorithm.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_threshold = 0.20\n",
    "verbose1 = False\n",
    "verbose2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e6ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# ~40 mins for 311k paragraphs\n",
    "\n",
    "topics = [key for key in list(sorted(mean_vectors_dict.keys()))]\n",
    "nested_l = [['index']+topics+['embedding_dominant']]\n",
    "\n",
    "for idx, row in tqdm(sample['merged_POS'].iteritems(), total=len(sample['merged_POS'])):\n",
    "    output = categorize_text(lemmatized_wordlist=row, mean_vectors_dict=mean_vectors_dict, keywords=topics, embeddings_dict=embeddings_dict, bottom_threshold=bottom_threshold, verbose1=verbose1, verbose2=verbose2)\n",
    "    \n",
    "    temp_l = [idx] +[result[1] for result in output['category_similarities']] + [output['prediction']]\n",
    "    \n",
    "    #print(row, output['category_similarities'])\n",
    "    \n",
    "    if len(nested_l[0]) != len(temp_l):\n",
    "        raise Exception('Not the same size!')\n",
    "        \n",
    "    nested_l.append(temp_l)\n",
    "\n",
    "\n",
    "prediction_df = pd.DataFrame(nested_l[1:],columns=nested_l[0]).set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b048b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df['embedding_dominant'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68a1b4",
   "metadata": {},
   "source": [
    "### 6. Insert classification into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = sample.join(prediction_df)\n",
    "updated_df.head(2)\n",
    "updated_df['same_categorisation'] = updated_df.apply(lambda x: x.lda_dominant.endswith(x.embedding_dominant), axis=1) # (updated_df['embedding_dominant'].isin('lda_dominant') 'lda_dominant'].str.contains() == updated_df['outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8d44f",
   "metadata": {},
   "source": [
    "## Saving \"Classified Paragraphs\" Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f626d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65616871",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_csv('..\\..\\..\\..\\..\\data\\clean\\lda_classified_30cities_435citypairs_311k_paragraphs_both_methods_with_education.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c8c90d",
   "metadata": {},
   "source": [
    "### 7. Informatics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287c9e4",
   "metadata": {},
   "source": [
    "#### Similarity between lda topic model and word embedding algorithm output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b594eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_categorisation = updated_df[updated_df['same_categorisation'] == False]['same_categorisation'].count()\n",
    "total_documents = updated_df['same_categorisation'].count()\n",
    "\n",
    "print(f\"{same_categorisation} out of {total_documents} were classified the same by the LDA topic model and word embedding classification model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(updated_df[(updated_df['lda_dominant_score'] > 0.9) & (updated_df['merged_POS'].str.len() > 5)].groupby('lda_dominant')['same_categorisation'].value_counts())\n",
    "print('----------------------------------------------------')\n",
    "print(updated_df[(updated_df['lda_dominant_score'] > 0.9) & (updated_df['merged_POS'].str.len() > 5)].groupby('lda_dominant')['same_categorisation'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97291575",
   "metadata": {},
   "source": [
    "#### binned LDA scores of all & differently classified paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df['lda_dominant_score'].value_counts(bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]).sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ddc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df[updated_df['same_categorisation'] != True]['lda_dominant_score'].value_counts(bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]).sort_index(ascending=False) #  .sum() # .sort_index(ascending=False) #.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71df061",
   "metadata": {},
   "source": [
    "#### Percentage of similarly classified documents between LDA topic modeling and word embedding classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802895b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = sorted(['sport', 'art', 'diplomacy', 'education', 'entertainment', 'transportation'])\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "topic_values = []\n",
    "for topic in topics:\n",
    "    values = []\n",
    "    for threshold in thresholds:\n",
    "        #x.lda_dominant.endswith(x.embedding_dominant)\n",
    "        values.append(updated_df[(updated_df['lda_dominant'].str.endswith(topic)) & (updated_df['lda_dominant_score'] > threshold) & (updated_df['merged_POS'].str.len() > 10)]['same_categorisation'].value_counts(normalize=True)[1])\n",
    "        #df[(df['idxmax'] == )  & (df['max'] > 0.8)]['outcome'].value_counts(normalize=True)\n",
    "    topic_values.append(values)\n",
    "\n",
    "topic_values\n",
    "new_topic_values = [[] for x in topic_values[0]]\n",
    "for index, topic in enumerate(topic_values):\n",
    "    for i, value in enumerate(topic):\n",
    "        new_topic_values[i].append(value)\n",
    "print(new_topic_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59520b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 10), facecolor=\"w\")\n",
    "# set width of bars\n",
    "barWidth = 0.25\n",
    "\n",
    "r1 =  np.arange(0, len(topic_values[0])+3, 1.50)\n",
    "# r1 = np.arange(0, len(topic_values[0])+1.5, 1.50)\n",
    "colors =[\"#54bebe\", \"#76c8c8\", \"#98d1d1\", \"#badbdb\", \"#97d2fb\", \"#eccbd9\", \"#eccbd9\"]\n",
    "colors = [ \"#badbdb\",\"#98d1d1\", \"#76c8c8\",\"#54bebe\", \"#63a4da\", \"#296ead\", \"#2a4693\"]\n",
    "\n",
    "for idx, topic in enumerate(new_topic_values):\n",
    "    plt.bar(r1, topic, color=colors[idx], width=barWidth, edgecolor='white', label=thresholds[idx], zorder=3)\n",
    "    r1= [x + barWidth for x in r1]\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.ylabel('Percentage (%)', fontweight='normal')\n",
    "plt.xlabel('Topic')\n",
    "plt.xticks([0.5, 2, 3.5,  5, 6.5, 8], ['Art', 'Diplomacy', 'Education', 'Entertainment', 'Sport', 'Transportation'])\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(-0.2, 9)\n",
    "\n",
    "# Create legend & Show graphic\n",
    "plt.title('Percentage of similarly classified documents by LDA topic modeling and word embeddings')\n",
    "plt.legend(title=\"threshold value\", loc=\"lower right\")\n",
    "plt.grid(zorder=0, color='lightgray', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df62ff",
   "metadata": {},
   "source": [
    "### 8. Aggregate paragraphs Classification into City Pair Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae626faa",
   "metadata": {},
   "source": [
    "#### 8.1 Select right parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_lda_threshold = 0.7\n",
    "minimal_paragraph_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires city1/city2 update!\n",
    "temp_df = updated_df[(updated_df['city_pair'].isin(updated_df['city_pair'].unique()[:])) & (updated_df['lda_dominant_score'] > bottom_lda_threshold) & (updated_df['merged_POS'].str.len() > minimal_paragraph_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa37ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91252aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = temp_df.groupby('city_pair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list2 = [['city_pair', 'paragraphs', 'lemmatised_paragraph_length', 'same_categorisation_raw', 'same_categorisation_percentage',\n",
    "                'lda_dominant_category', 'embedding_dominant_category', 'lda_art', 'embedding_art', 'lda_diplomacy',\n",
    "                'embedding_diplomacy', 'lda_education', 'embedding_education', 'lda_entertainment', 'embedding_entertainment', \n",
    "                'lda_sport', 'embedding_sport', 'lda_transportation', 'embedding_transportation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bcc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ca3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(temp_df['embedding_dominant'].unique()))\n",
    "print(list(temp_df['lda_dominant'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1aba73",
   "metadata": {},
   "source": [
    "#### 8.2 Aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3084fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "lda_categories = list(temp_df['lda_dominant'].unique())\n",
    "embedding_categories = list(temp_df['embedding_dominant'].unique())\n",
    "\n",
    "for city_pair, sub_df in tqdm(grouped_df):\n",
    "    paragraph_count = sub_df['paragraph'].count()\n",
    "    same_categorisation_raw = sub_df['same_categorisation'].sum()\n",
    "    same_categorisation_percentage = sub_df['same_categorisation'].sum()/sub_df['same_categorisation'].count()\n",
    "    lemmatised_paragraph_len = sub_df['merged_POS'].str.len().mean()\n",
    "    \n",
    "    lda_prediction = sub_df['lda_dominant'].value_counts()\n",
    "    embedding_prediction = sub_df['embedding_dominant'].value_counts()\n",
    "        \n",
    "    lda_dominant_category = lda_prediction.idxmax()\n",
    "    embedding_dominant_category = embedding_prediction.idxmax()\n",
    "    \n",
    "    lda_prediction = lda_prediction.to_dict()\n",
    "    embedding_prediction = embedding_prediction.to_dict()\n",
    "    \n",
    "    if (len(lda_prediction) != len(lda_categories)):\n",
    "        for category in lda_categories:\n",
    "            if category not in lda_prediction.keys():\n",
    "                lda_prediction[category] = 0\n",
    "                \n",
    "    if (len(embedding_prediction) != len(embedding_categories)):\n",
    "        for category in embedding_categories:\n",
    "            if category not in embedding_prediction.keys():\n",
    "                embedding_prediction[category] = 0\n",
    "                \n",
    "    temp_l2 = [city_pair, paragraph_count, lemmatised_paragraph_len, same_categorisation_raw, same_categorisation_percentage,\n",
    "                lda_dominant_category, embedding_dominant_category, lda_prediction['lda_art'], embedding_prediction['art'], lda_prediction['lda_diplomacy'],\n",
    "                embedding_prediction['diplomacy'], lda_prediction['lda_education'], embedding_prediction['education'],\n",
    "                lda_prediction['lda_entertainment'], embedding_prediction['entertainment'], lda_prediction['lda_sport'],\n",
    "                embedding_prediction['sport'], lda_prediction['lda_transportation'], embedding_prediction['transportation']]\n",
    "    if len(nested_list2[0]) != len(temp_l2):\n",
    "        raise Exception('Not the same size!')\n",
    "    \n",
    "    nested_list2.append(temp_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6a262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(nested_list2[1:],columns=nested_list2[0])\n",
    "\n",
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa9d0ef",
   "metadata": {},
   "source": [
    "## 8.4 Save Aggregated Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e72a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.to_csv('..\\..\\..\\..\\..\\data\\clean\\deliverable_435city_pairs_both_methods_with_education_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bba1b",
   "metadata": {},
   "source": [
    "#### 8.5 Normalise classification (by number of paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce52a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize category outcomes\n",
    "final_df_normalised = final_df\n",
    "final_df_normalised[list(final_df_normalised.columns)[7:]] = final_df_normalised[list(final_df_normalised.columns)[7:]].div(final_df_normalised['paragraphs'], axis=0) # .count()\n",
    "\n",
    "final_df_normalised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd13ea95",
   "metadata": {},
   "source": [
    "## 8.6 Save Normalised Aggregated Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd22076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df_normalised.to_csv('..\\..\\..\\..\\..\\data\\clean\\deliverable_435city_pairs_both_methods_with_education_final_normalised.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174a210",
   "metadata": {},
   "source": [
    "# EXTRAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23815b5",
   "metadata": {},
   "source": [
    "#### Show closest words to topic vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from preprocessing_functions import *\n",
    "\n",
    "def find_closest_embeddings(embedding, cutoff=25):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda token: spatial.distance.euclidean(embeddings_dict[token], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca79a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'art'\n",
    "\n",
    "words = find_closest_embeddings(embedding=\n",
    "     mean_vectors_dict[topic]    # embeddings_dict['diplomacy'] # embeddings_dict['fashion']\n",
    ")[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = get_english_words(path='../../../input/english_words_alpha_370k.txt\n",
    "print(remove_non_existing_words_from_wordlist(words, english_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
