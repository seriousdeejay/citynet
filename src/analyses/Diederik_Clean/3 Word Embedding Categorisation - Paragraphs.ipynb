{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c643808",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; border-radius: 5px; padding: 10px;\">\n",
    "    <h4>Word Embedding Categorisation</h4>\n",
    "    <p>...</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62a4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_functions import *\n",
    "from word_embedding_functions import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e356c",
   "metadata": {},
   "source": [
    "- Clean and preprocess (lemmatise) a list of documents (e.g. paragraphs)\n",
    "- Get your topics through unsupervised clustering with (LDA Topic Modeling)\n",
    "- Use these for the word embedding algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127441f",
   "metadata": {},
   "source": [
    "### 1. Set Topic Words\n",
    "<br>\n",
    "<div style=\"text-align: justify;\">Using LDA topic modeling to find topics and their words through unsupervised clustering highly increases the performance of the model. Look at the pyLDAvis visualisation of the LDA topic model and change any parameters if you are not happy with the cluster distributions or if any clusters are overlapping. </div>\n",
    "<br>\n",
    "<div style=\"text-align: justify;\">\n",
    "A good topic model will have relatively big, similarly sized and non-overlapping bubbles scattered throughout the chart. Greater distances between the clusters represents a larger semantic difference, similarly sized bubbles show that the topics are equally represented, and large circles mean that the topics are well represented in the documents. By paying attention to these three characteristics we can get an accurate representation of the dominant topics of our documents and decide whether these clusters represent good, meaningful topics.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953a77a",
   "metadata": {},
   "source": [
    "#### 1.1 Option 1 (less optimal!): Select individual words for each category/topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "422727f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you want to set your own topic words instead of using the recommended lda topic words? (y/n)n\n",
      "Canceling cell execution...\n"
     ]
    }
   ],
   "source": [
    "# Removing words with double meaning helps (e.g. bear, degree)\n",
    "\n",
    "choice = input('Are you sure you want to set your own topic words instead of using the recommended lda topic words? (y/n)')\n",
    "\n",
    "if choice == 'y':\n",
    "    print('Running cell...')\n",
    "    \n",
    "    #fashion_words = ['fashion', 'model', 'vogue', 'store', 'designer', 'couture', 'catwalk', 'runway', 'modeling', 'clothing', 'cosmetic', 'brand', 'retail', 'advertising', 'perfume']\n",
    "    diplomacy_words = ['War', 'Embassy', 'Army', 'Diplomatic', 'Ambassador', 'Treaty', 'Protest', 'Force', 'Mission', 'Arrest', 'Government', 'Police', 'Attack', 'Party', 'Minister']\n",
    "    entertainment_words = ['Opera', 'Festival', 'Perform', 'Orchestra', 'Symphony', 'Concert', 'Music', 'Film', 'Sing', 'Theatre', 'Performance', 'Role', 'Premiere', 'Tour', 'Band']\n",
    "    art_words = ['Exhibition', 'Art', 'Museum', 'Gallery', 'Exhibit', 'Painting', 'Collection', 'Paint', 'Portrait', 'Artist', 'Sculpture', 'Fashion', 'Design', 'Contemporary', 'Painter']\n",
    "    education_words = ['Study', 'School', 'Professor', 'University', 'Graduate', 'Educate', 'Lecture', 'Research', 'College', 'Teach', 'Science', 'Education', 'Philosophy', 'Doctorate', 'Faculty'] # replaced bear with educate and degree with Lecture\n",
    "    transportation_words = ['Railway', 'Route', 'Line', 'Operate', 'Flight', 'Station', 'Service', 'Airline', 'Airport', 'Train', 'Passenger', 'Speed', 'Aircraft', 'Rail', 'Network']\n",
    "    sport_words = ['Final', 'Win', 'Team', 'Match', 'Game', 'Goal', 'Club', 'League', 'Champion', 'Championship', 'Season', 'Score', 'Round', 'Tournament', 'Football']\n",
    "\n",
    "    lda_topic_words = { #'fashion': fashion_words,\n",
    "                        'diplomacy': diplomacy_words,\n",
    "                       'entertainment': entertainment_words,\n",
    "                       'art': art_words,\n",
    "                       'education': education_words,\n",
    "                       'transportation': transportation_words,\n",
    "                       'sport': sport_words}\n",
    "else:\n",
    "    print('Canceling cell execution...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40317b97",
   "metadata": {},
   "source": [
    "#### 1.2 Option 2 (recommended!): Use Topic Model's topic words (found through LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3887a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_words_df_path = '../../../../data_clean/lda_models/lda_model_2million/relevant_words_per_topic.csv'\n",
    "lda_topic_words_df = pd.read_csv(lda_topic_words_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f60fc568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perform</td>\n",
       "      <td>route</td>\n",
       "      <td>store</td>\n",
       "      <td>season</td>\n",
       "      <td>study</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>opera</td>\n",
       "      <td>station</td>\n",
       "      <td>building</td>\n",
       "      <td>win</td>\n",
       "      <td>exhibition</td>\n",
       "      <td>protest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>concert</td>\n",
       "      <td>service</td>\n",
       "      <td>office</td>\n",
       "      <td>club</td>\n",
       "      <td>professor</td>\n",
       "      <td>army</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>film</td>\n",
       "      <td>railway</td>\n",
       "      <td>design</td>\n",
       "      <td>goal</td>\n",
       "      <td>work</td>\n",
       "      <td>force</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tour</td>\n",
       "      <td>line</td>\n",
       "      <td>population</td>\n",
       "      <td>match</td>\n",
       "      <td>painting</td>\n",
       "      <td>arrest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sing</td>\n",
       "      <td>train</td>\n",
       "      <td>firm</td>\n",
       "      <td>team</td>\n",
       "      <td>bear</td>\n",
       "      <td>troop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>music</td>\n",
       "      <td>passenger</td>\n",
       "      <td>century</td>\n",
       "      <td>score</td>\n",
       "      <td>exhibit</td>\n",
       "      <td>police</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>premiere</td>\n",
       "      <td>airline</td>\n",
       "      <td>shopping</td>\n",
       "      <td>game</td>\n",
       "      <td>school</td>\n",
       "      <td>command</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>festival</td>\n",
       "      <td>airport</td>\n",
       "      <td>large</td>\n",
       "      <td>final</td>\n",
       "      <td>graduate</td>\n",
       "      <td>war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>band</td>\n",
       "      <td>operate</td>\n",
       "      <td>urban</td>\n",
       "      <td>league</td>\n",
       "      <td>art</td>\n",
       "      <td>embassy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>orchestra</td>\n",
       "      <td>speed</td>\n",
       "      <td>city</td>\n",
       "      <td>player</td>\n",
       "      <td>degree</td>\n",
       "      <td>mission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>performance</td>\n",
       "      <td>flight</td>\n",
       "      <td>industry</td>\n",
       "      <td>defeat</td>\n",
       "      <td>painter</td>\n",
       "      <td>consulate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>role</td>\n",
       "      <td>rail</td>\n",
       "      <td>site</td>\n",
       "      <td>beat</td>\n",
       "      <td>teach</td>\n",
       "      <td>diplomatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>album</td>\n",
       "      <td>connect</td>\n",
       "      <td>tower</td>\n",
       "      <td>finish</td>\n",
       "      <td>father</td>\n",
       "      <td>bomb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>song</td>\n",
       "      <td>km</td>\n",
       "      <td>headquarter</td>\n",
       "      <td>football</td>\n",
       "      <td>publish</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>conductor</td>\n",
       "      <td>destination</td>\n",
       "      <td>fashion</td>\n",
       "      <td>victory</td>\n",
       "      <td>portrait</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>production</td>\n",
       "      <td>traffic</td>\n",
       "      <td>housing</td>\n",
       "      <td>round</td>\n",
       "      <td>doctorate</td>\n",
       "      <td>raid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>theatre</td>\n",
       "      <td>locomotive</td>\n",
       "      <td>market</td>\n",
       "      <td>play</td>\n",
       "      <td>educate</td>\n",
       "      <td>arrive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>singer</td>\n",
       "      <td>seat</td>\n",
       "      <td>area</td>\n",
       "      <td>draw</td>\n",
       "      <td>paint</td>\n",
       "      <td>commander</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>musical</td>\n",
       "      <td>tram</td>\n",
       "      <td>location</td>\n",
       "      <td>lose</td>\n",
       "      <td>university</td>\n",
       "      <td>capture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Topic 1      Topic 2      Topic 3   Topic 4     Topic 5     Topic 6\n",
       "0       perform        route        store    season       study      attack\n",
       "1         opera      station     building       win  exhibition     protest\n",
       "2       concert      service       office      club   professor        army\n",
       "3          film      railway       design      goal        work       force\n",
       "4          tour         line   population     match    painting      arrest\n",
       "5          sing        train         firm      team        bear       troop\n",
       "6         music    passenger      century     score     exhibit      police\n",
       "7      premiere      airline     shopping      game      school     command\n",
       "8      festival      airport        large     final    graduate         war\n",
       "9          band      operate        urban    league         art     embassy\n",
       "10    orchestra        speed         city    player      degree     mission\n",
       "11  performance       flight     industry    defeat     painter   consulate\n",
       "12         role         rail         site      beat       teach  diplomatic\n",
       "13        album      connect        tower    finish      father        bomb\n",
       "14         song           km  headquarter  football     publish  government\n",
       "15    conductor  destination      fashion   victory    portrait        kill\n",
       "16   production      traffic      housing     round   doctorate        raid\n",
       "17      theatre   locomotive       market      play     educate      arrive\n",
       "18       singer         seat         area      draw       paint   commander\n",
       "19      musical         tram     location      lose  university     capture"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(lda_topic_words_df[:20])\n",
    "len(lda_topic_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e8084aeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ['perform', 'opera', 'concert', 'film', 'tour', 'sing', 'music', 'premiere', 'festival', 'band'] \n",
      "\n",
      "Topic 2: ['route', 'station', 'service', 'railway', 'line', 'train', 'passenger', 'airline', 'airport', 'operate'] \n",
      "\n",
      "Topic 3: ['store', 'building', 'office', 'design', 'population', 'firm', 'century', 'shopping', 'large', 'urban'] \n",
      "\n",
      "Topic 4: ['season', 'win', 'club', 'goal', 'match', 'team', 'score', 'game', 'final', 'league'] \n",
      "\n",
      "Topic 5: ['study', 'exhibition', 'professor', 'work', 'painting', 'bear', 'exhibit', 'school', 'graduate', 'art'] \n",
      "\n",
      "Topic 6: ['attack', 'protest', 'army', 'force', 'arrest', 'troop', 'police', 'command', 'war', 'embassy'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nr_of_words = 10\n",
    "lda_topic_words = lda_topic_words_df[:nr_of_words].to_dict('list')\n",
    "\n",
    "for key in lda_topic_words: # sorted_df_dict = sorted(df_dict, key=lambda d: d['article_id']) \n",
    "    print(f\"{key}:\", lda_topic_words[key], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2195b8",
   "metadata": {},
   "source": [
    "### 2. Loading GloVe Word Embedding\n",
    "<br>\n",
    "<div style=\"text-align: justify;\"> The word embedding model could be replaced by other ones fairly easy, but for now we decided to use the Common Crawl 840B token one  (source: https://nlp.stanford.edu/projects/glove/).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "05c59a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take approximately ~ 4 minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6f0402d25c4b26ab73c3aa63dd0ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2196017 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 38s\n",
      "Wall time: 3min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embeddings_dict, discarded_dict = load_glove_word_embeddings(GLOVE_PATH=\"../../../../glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63be62",
   "metadata": {},
   "source": [
    "### 3. Get Mean of the words from each Topic\n",
    "<br>\n",
    "<div style=\"text-align: justify;\">By taking the mean of the vectors that belong to the ~15 most relevant words of a topic we get a fairly accurate vector representation of a topic. We do this for each topic and save their vector value to a dictionary for later retrieval.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "16e65d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vectors_dict = {}\n",
    "\n",
    "for topic in lda_topic_words:\n",
    "    words = lda_topic_words[topic]\n",
    "    words = [word for word in words if word in embeddings_dict.keys()] # checks if word is in vocabulary (i.e. has been seen by the model before)\n",
    "    mean_embedding = np.mean([embeddings_dict[word.lower()] for word in words], axis=0)\n",
    "    mean_vectors_dict[topic] = mean_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98718a8d",
   "metadata": {},
   "source": [
    "### 4. Load Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea14cd4",
   "metadata": {},
   "source": [
    "#### 4.1 Load and merge chunked .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8af53cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_with_lda_prediction_path = '../../../../data_clean/lda_models/lda_model_2million/paragraphs_lda_topic_distribution/'\n",
    "file_path =  os.listdir(paragraphs_with_lda_prediction_path)[0]\n",
    "sample_path = os.path.join(paragraphs_with_lda_prediction_path, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6a6cea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraphs_10_934384_1038204_merged_POS.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.listdir('../../../../data_clean/lda_models/lda_model_2million/paragraphs_lda_topic_distribution/')[0]\n",
    "lemmatised_paragraphs_path = \"../../../../data_clean/paragraphs_lemmatised/english_words_merged_NOUNVERBADJ/\"\n",
    "file_path = os.listdir(lemmatised_paragraphs_path)[0]\n",
    "sample_path = os.path.join(lemmatised_paragraphs_path, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d620069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../../data_clean/lda_models/lda_model_2million/paragraphs_lda_topic_distribution/paragraphs_10_934385_1038205_lda_topics.csv paragraphs_10_934385_1038205_lda_topics.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sample_path, file_path)\n",
    "os.path.exists(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7d189714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_1</th>\n",
       "      <th>city_2</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>merged_POS</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>lda_dominant</th>\n",
       "      <th>lda_dominant_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toulon</td>\n",
       "      <td>Nice</td>\n",
       "      <td>Honey from Provence is protected by a red labe...</td>\n",
       "      <td>15415861.0</td>\n",
       "      <td>Authon, Alpes-de-Haute-Provence</td>\n",
       "      <td>['honey', 'label', 'indication', 'honey', 'flo...</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.917025</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>topic_5</td>\n",
       "      <td>0.917025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Copenhagen</td>\n",
       "      <td>Oslo</td>\n",
       "      <td>Glosimodt was born in Oslo, Norway. He studied...</td>\n",
       "      <td>15416040.0</td>\n",
       "      <td>Erik Glosimodt</td>\n",
       "      <td>['education', 'professor', 'architect', 'pract...</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.987215</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>topic_5</td>\n",
       "      <td>0.987215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oslo</td>\n",
       "      <td>Copenhagen</td>\n",
       "      <td>Glosimodt was born in Oslo, Norway. He studied...</td>\n",
       "      <td>15416040.0</td>\n",
       "      <td>Erik Glosimodt</td>\n",
       "      <td>['education', 'professor', 'architect', 'pract...</td>\n",
       "      <td>0.018396</td>\n",
       "      <td>0.901826</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>0.015259</td>\n",
       "      <td>0.028899</td>\n",
       "      <td>0.017797</td>\n",
       "      <td>topic_2</td>\n",
       "      <td>0.901826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Dusseldorf</td>\n",
       "      <td>On the 24 March 2015, Germanwings Flight 9525 ...</td>\n",
       "      <td>15416113.0</td>\n",
       "      <td>Meolans-Revel</td>\n",
       "      <td>['travel', 'crash']</td>\n",
       "      <td>0.018396</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>0.905299</td>\n",
       "      <td>0.028899</td>\n",
       "      <td>0.017797</td>\n",
       "      <td>topic_4</td>\n",
       "      <td>0.905299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dusseldorf</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>On the 24 March 2015, Germanwings Flight 9525 ...</td>\n",
       "      <td>15416113.0</td>\n",
       "      <td>Meolans-Revel</td>\n",
       "      <td>['travel', 'crash']</td>\n",
       "      <td>0.005012</td>\n",
       "      <td>0.973250</td>\n",
       "      <td>0.004856</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>0.004849</td>\n",
       "      <td>topic_2</td>\n",
       "      <td>0.973250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103815</th>\n",
       "      <td>Rome</td>\n",
       "      <td>Bari</td>\n",
       "      <td>It publishes 15 local editions for the cities ...</td>\n",
       "      <td>19593000.0</td>\n",
       "      <td>Leggo</td>\n",
       "      <td>['edition', 'city', 'circulation', 'copy', 'pu...</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.971431</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>topic_3</td>\n",
       "      <td>0.971431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103816</th>\n",
       "      <td>Rome</td>\n",
       "      <td>Padua</td>\n",
       "      <td>It publishes 15 local editions for the cities ...</td>\n",
       "      <td>19593000.0</td>\n",
       "      <td>Leggo</td>\n",
       "      <td>['edition', 'city', 'circulation', 'copy', 'pu...</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.971431</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>topic_3</td>\n",
       "      <td>0.971431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103817</th>\n",
       "      <td>Rome</td>\n",
       "      <td>Brescia</td>\n",
       "      <td>It publishes 15 local editions for the cities ...</td>\n",
       "      <td>19593000.0</td>\n",
       "      <td>Leggo</td>\n",
       "      <td>['edition', 'city', 'circulation', 'copy', 'pu...</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.833445</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>topic_3</td>\n",
       "      <td>0.833445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103818</th>\n",
       "      <td>Rome</td>\n",
       "      <td>Verona</td>\n",
       "      <td>It publishes 15 local editions for the cities ...</td>\n",
       "      <td>19593000.0</td>\n",
       "      <td>Leggo</td>\n",
       "      <td>['edition', 'city', 'circulation', 'copy', 'pu...</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.971431</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>topic_3</td>\n",
       "      <td>0.971431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103819</th>\n",
       "      <td>Naples</td>\n",
       "      <td>Milan</td>\n",
       "      <td>It publishes 15 local editions for the cities ...</td>\n",
       "      <td>19593000.0</td>\n",
       "      <td>Leggo</td>\n",
       "      <td>['edition', 'city', 'circulation', 'copy', 'pu...</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.971431</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>topic_3</td>\n",
       "      <td>0.971431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103820 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            city_1      city_2  \\\n",
       "0           Toulon        Nice   \n",
       "1       Copenhagen        Oslo   \n",
       "2             Oslo  Copenhagen   \n",
       "3        Barcelona  Dusseldorf   \n",
       "4       Dusseldorf   Barcelona   \n",
       "...            ...         ...   \n",
       "103815        Rome        Bari   \n",
       "103816        Rome       Padua   \n",
       "103817        Rome     Brescia   \n",
       "103818        Rome      Verona   \n",
       "103819      Naples       Milan   \n",
       "\n",
       "                                                paragraph  article_id  \\\n",
       "0       Honey from Provence is protected by a red labe...  15415861.0   \n",
       "1       Glosimodt was born in Oslo, Norway. He studied...  15416040.0   \n",
       "2       Glosimodt was born in Oslo, Norway. He studied...  15416040.0   \n",
       "3       On the 24 March 2015, Germanwings Flight 9525 ...  15416113.0   \n",
       "4       On the 24 March 2015, Germanwings Flight 9525 ...  15416113.0   \n",
       "...                                                   ...         ...   \n",
       "103815  It publishes 15 local editions for the cities ...  19593000.0   \n",
       "103816  It publishes 15 local editions for the cities ...  19593000.0   \n",
       "103817  It publishes 15 local editions for the cities ...  19593000.0   \n",
       "103818  It publishes 15 local editions for the cities ...  19593000.0   \n",
       "103819  It publishes 15 local editions for the cities ...  19593000.0   \n",
       "\n",
       "                                  title  \\\n",
       "0       Authon, Alpes-de-Haute-Provence   \n",
       "1                        Erik Glosimodt   \n",
       "2                        Erik Glosimodt   \n",
       "3                         Meolans-Revel   \n",
       "4                         Meolans-Revel   \n",
       "...                                 ...   \n",
       "103815                            Leggo   \n",
       "103816                            Leggo   \n",
       "103817                            Leggo   \n",
       "103818                            Leggo   \n",
       "103819                            Leggo   \n",
       "\n",
       "                                               merged_POS   topic_1   topic_2  \\\n",
       "0       ['honey', 'label', 'indication', 'honey', 'flo...  0.002901  0.072049   \n",
       "1       ['education', 'professor', 'architect', 'pract...  0.002901  0.001859   \n",
       "2       ['education', 'professor', 'architect', 'pract...  0.018396  0.901826   \n",
       "3                                     ['travel', 'crash']  0.018396  0.011786   \n",
       "4                                     ['travel', 'crash']  0.005012  0.973250   \n",
       "...                                                   ...       ...       ...   \n",
       "103815  ['edition', 'city', 'circulation', 'copy', 'pu...  0.005704  0.003654   \n",
       "103816  ['edition', 'city', 'circulation', 'copy', 'pu...  0.005704  0.003654   \n",
       "103817  ['edition', 'city', 'circulation', 'copy', 'pu...  0.005704  0.003654   \n",
       "103818  ['edition', 'city', 'circulation', 'copy', 'pu...  0.005704  0.003654   \n",
       "103819  ['edition', 'city', 'circulation', 'copy', 'pu...  0.005704  0.003654   \n",
       "\n",
       "         topic_3   topic_4   topic_5   topic_6 lda_dominant  \\\n",
       "0       0.002811  0.002407  0.917025  0.002807      topic_5   \n",
       "1       0.002811  0.002407  0.987215  0.002807      topic_5   \n",
       "2       0.017823  0.015259  0.028899  0.017797      topic_2   \n",
       "3       0.017823  0.905299  0.028899  0.017797      topic_4   \n",
       "4       0.004856  0.004158  0.007874  0.004849      topic_2   \n",
       "...          ...       ...       ...       ...          ...   \n",
       "103815  0.971431  0.004731  0.008961  0.005518      topic_3   \n",
       "103816  0.971431  0.004731  0.008961  0.005518      topic_3   \n",
       "103817  0.833445  0.004731  0.146947  0.005518      topic_3   \n",
       "103818  0.971431  0.004731  0.008961  0.005518      topic_3   \n",
       "103819  0.971431  0.004731  0.008961  0.005518      topic_3   \n",
       "\n",
       "        lda_dominant_score  \n",
       "0                 0.917025  \n",
       "1                 0.987215  \n",
       "2                 0.901826  \n",
       "3                 0.905299  \n",
       "4                 0.973250  \n",
       "...                    ...  \n",
       "103815            0.971431  \n",
       "103816            0.971431  \n",
       "103817            0.833445  \n",
       "103818            0.971431  \n",
       "103819            0.971431  \n",
       "\n",
       "[103820 rows x 14 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.read_csv(sample_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861ea5c",
   "metadata": {},
   "source": [
    "#### 4.2 Select number of paragraphs to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c188296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 5000\n",
    "sample = df[:sample_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234aa540",
   "metadata": {},
   "source": [
    "#### 4.3 Turn stringed lists (in csv) into list objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6e2212c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 250 ms\n",
      "Wall time: 395 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "    \n",
    "sample['merged_POS'] = sample['merged_POS'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a82403b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['city_1', 'city_2', 'paragraph', 'article_id', 'title', 'merged_POS',\n",
       "       'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6',\n",
       "       'lda_dominant', 'lda_dominant_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6275204d",
   "metadata": {},
   "source": [
    "### 5. Classify Paragraphs (by Word Embedding Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9c479",
   "metadata": {},
   "source": [
    "#### 5.1 Select right parameters\n",
    "<br>\n",
    "<div style=\"text-align: justify;\">bottom_threshold: If the similarity between a word and its closest topic is below the bottom_threshold it will be discarded from the classification process.\n",
    "Verbose1 and Verbose2: Enabling these will print out the internal process of the algorithm.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4667ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_threshold = 0.20\n",
    "verbose1 = False\n",
    "verbose2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b34e6ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5de60ab8f0e44bab27b8f39853e72ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 30.2 s\n",
      "Wall time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ~40 mins for 311k paragraphs\n",
    "\n",
    "topics = [key for key in list(sorted(mean_vectors_dict.keys()))]\n",
    "nested_l = [['index']+topics+['embedding_dominant']]\n",
    "\n",
    "for idx, row in tqdm(sample['merged_POS'].iteritems(), total=len(sample['merged_POS'])):\n",
    "    output = categorize_text(lemmatized_wordlist=row, mean_vectors_dict=mean_vectors_dict, keywords=topics, embeddings_dict=embeddings_dict, bottom_threshold=bottom_threshold, verbose1=verbose1, verbose2=verbose2)\n",
    "    \n",
    "    temp_l = [idx] +[result[1] for result in output['category_similarities']] + [output['prediction']]\n",
    "    \n",
    "    #print(row, output['category_similarities'])\n",
    "    \n",
    "    if len(nested_l[0]) != len(temp_l):\n",
    "        raise Exception('Not the same size!')\n",
    "        \n",
    "    nested_l.append(temp_l)\n",
    "\n",
    "\n",
    "prediction_df = pd.DataFrame(nested_l[1:],columns=nested_l[0]).set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2a387bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>embedding_dominant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.177238</td>\n",
       "      <td>3.370153</td>\n",
       "      <td>7.421871</td>\n",
       "      <td>1.268505</td>\n",
       "      <td>1.032050</td>\n",
       "      <td>1.251641</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.535294</td>\n",
       "      <td>1.509655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.934449</td>\n",
       "      <td>0.763062</td>\n",
       "      <td>Topic 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.535294</td>\n",
       "      <td>1.509655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.934449</td>\n",
       "      <td>0.763062</td>\n",
       "      <td>Topic 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.578503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417536</td>\n",
       "      <td>Topic 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.578503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417536</td>\n",
       "      <td>Topic 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.398827</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>6.762273</td>\n",
       "      <td>0.375313</td>\n",
       "      <td>5.861372</td>\n",
       "      <td>1.856807</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.398827</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>6.762273</td>\n",
       "      <td>0.375313</td>\n",
       "      <td>5.861372</td>\n",
       "      <td>1.856807</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.398827</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>6.762273</td>\n",
       "      <td>0.375313</td>\n",
       "      <td>5.861372</td>\n",
       "      <td>1.856807</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.398827</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>6.762273</td>\n",
       "      <td>0.375313</td>\n",
       "      <td>5.861372</td>\n",
       "      <td>1.856807</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.398827</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>6.762273</td>\n",
       "      <td>0.375313</td>\n",
       "      <td>5.861372</td>\n",
       "      <td>1.856807</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Topic 1   Topic 2   Topic 3   Topic 4   Topic 5   Topic 6  \\\n",
       "index                                                               \n",
       "0      1.177238  3.370153  7.421871  1.268505  1.032050  1.251641   \n",
       "1      0.000000  1.535294  1.509655  0.000000  3.934449  0.763062   \n",
       "2      0.000000  1.535294  1.509655  0.000000  3.934449  0.763062   \n",
       "3      0.000000  0.578503  0.000000  0.000000  0.000000  0.417536   \n",
       "4      0.000000  0.578503  0.000000  0.000000  0.000000  0.417536   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "4995   0.398827  0.411773  6.762273  0.375313  5.861372  1.856807   \n",
       "4996   0.398827  0.411773  6.762273  0.375313  5.861372  1.856807   \n",
       "4997   0.398827  0.411773  6.762273  0.375313  5.861372  1.856807   \n",
       "4998   0.398827  0.411773  6.762273  0.375313  5.861372  1.856807   \n",
       "4999   0.398827  0.411773  6.762273  0.375313  5.861372  1.856807   \n",
       "\n",
       "      embedding_dominant  \n",
       "index                     \n",
       "0                Topic 3  \n",
       "1                Topic 5  \n",
       "2                Topic 5  \n",
       "3                Topic 2  \n",
       "4                Topic 2  \n",
       "...                  ...  \n",
       "4995             Topic 3  \n",
       "4996             Topic 3  \n",
       "4997             Topic 3  \n",
       "4998             Topic 3  \n",
       "4999             Topic 3  \n",
       "\n",
       "[5000 rows x 7 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "43b048b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic 3    1426\n",
       "Topic 5    1106\n",
       "Topic 4     968\n",
       "Topic 1     754\n",
       "Topic 2     408\n",
       "Topic 6     338\n",
       "Name: embedding_dominant, dtype: int64"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df['embedding_dominant'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68a1b4",
   "metadata": {},
   "source": [
    "### 6. Insert classification into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "fd8e6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = sample.join(prediction_df)\n",
    "updated_df.head(2)\n",
    "updated_df['same_categorisation'] = updated_df.apply(lambda x: x.lda_dominant.endswith(x.embedding_dominant[-1]), axis=1) # (updated_df['embedding_dominant'].isin('lda_dominant') 'lda_dominant'].str.contains() == updated_df['outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8d44f",
   "metadata": {},
   "source": [
    "## Saving \"Classified Paragraphs\" Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f626d5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_1</th>\n",
       "      <th>city_2</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>merged_POS</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>...</th>\n",
       "      <th>lda_dominant</th>\n",
       "      <th>lda_dominant_score</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>embedding_dominant</th>\n",
       "      <th>same_categorisation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toulon</td>\n",
       "      <td>Nice</td>\n",
       "      <td>Honey from Provence is protected by a red labe...</td>\n",
       "      <td>15415861.0</td>\n",
       "      <td>Authon, Alpes-de-Haute-Provence</td>\n",
       "      <td>[honey, label, indication, honey, flower, hone...</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>...</td>\n",
       "      <td>topic_5</td>\n",
       "      <td>0.917025</td>\n",
       "      <td>1.177238</td>\n",
       "      <td>3.370153</td>\n",
       "      <td>7.421871</td>\n",
       "      <td>1.268505</td>\n",
       "      <td>1.032050</td>\n",
       "      <td>1.251641</td>\n",
       "      <td>Topic 3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Copenhagen</td>\n",
       "      <td>Oslo</td>\n",
       "      <td>Glosimodt was born in Oslo, Norway. He studied...</td>\n",
       "      <td>15416040.0</td>\n",
       "      <td>Erik Glosimodt</td>\n",
       "      <td>[education, professor, architect, practice, tr...</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>...</td>\n",
       "      <td>topic_5</td>\n",
       "      <td>0.987215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.535294</td>\n",
       "      <td>1.509655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.934449</td>\n",
       "      <td>0.763062</td>\n",
       "      <td>Topic 5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oslo</td>\n",
       "      <td>Copenhagen</td>\n",
       "      <td>Glosimodt was born in Oslo, Norway. He studied...</td>\n",
       "      <td>15416040.0</td>\n",
       "      <td>Erik Glosimodt</td>\n",
       "      <td>[education, professor, architect, practice, tr...</td>\n",
       "      <td>0.018396</td>\n",
       "      <td>0.901826</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>0.015259</td>\n",
       "      <td>...</td>\n",
       "      <td>topic_2</td>\n",
       "      <td>0.901826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.535294</td>\n",
       "      <td>1.509655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.934449</td>\n",
       "      <td>0.763062</td>\n",
       "      <td>Topic 5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Dusseldorf</td>\n",
       "      <td>On the 24 March 2015, Germanwings Flight 9525 ...</td>\n",
       "      <td>15416113.0</td>\n",
       "      <td>Meolans-Revel</td>\n",
       "      <td>[travel, crash]</td>\n",
       "      <td>0.018396</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>0.905299</td>\n",
       "      <td>...</td>\n",
       "      <td>topic_4</td>\n",
       "      <td>0.905299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.578503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417536</td>\n",
       "      <td>Topic 2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dusseldorf</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>On the 24 March 2015, Germanwings Flight 9525 ...</td>\n",
       "      <td>15416113.0</td>\n",
       "      <td>Meolans-Revel</td>\n",
       "      <td>[travel, crash]</td>\n",
       "      <td>0.005012</td>\n",
       "      <td>0.973250</td>\n",
       "      <td>0.004856</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>...</td>\n",
       "      <td>topic_2</td>\n",
       "      <td>0.973250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.578503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417536</td>\n",
       "      <td>Topic 2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Rome</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>He was sent to the Cape of Good Hope with lett...</td>\n",
       "      <td>15638655.0</td>\n",
       "      <td>Peter Kolbe</td>\n",
       "      <td>[letter, introduction, mayor, mandate, descrip...</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.421843</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>...</td>\n",
       "      <td>topic_5</td>\n",
       "      <td>0.575438</td>\n",
       "      <td>0.398827</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>6.762273</td>\n",
       "      <td>0.375313</td>\n",
       "      <td>5.861372</td>\n",
       "      <td>1.856807</td>\n",
       "      <td>Topic 3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Rome</td>\n",
       "      <td>Nuremberg</td>\n",
       "      <td>He was sent to the Cape of Good Hope with lett...</td>\n",
       "      <td>15638655.0</td>\n",
       "      <td>Peter Kolbe</td>\n",
       "      <td>[letter, introduction, mayor, mandate, descrip...</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.192165</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>...</td>\n",
       "      <td>topic_5</td>\n",
       "      <td>0.805115</td>\n",
       "      <td>0.398827</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>6.762273</td>\n",
       "      <td>0.375313</td>\n",
       "      <td>5.861372</td>\n",
       "      <td>1.856807</td>\n",
       "      <td>Topic 3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>Rome</td>\n",
       "      <td>He was sent to the Cape of Good Hope with lett...</td>\n",
       "      <td>15638655.0</td>\n",
       "      <td>Peter Kolbe</td>\n",
       "      <td>[letter, introduction, mayor, mandate, descrip...</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.268724</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>...</td>\n",
       "      <td>topic_5</td>\n",
       "      <td>0.728556</td>\n",
       "      <td>0.398827</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>6.762273</td>\n",
       "      <td>0.375313</td>\n",
       "      <td>5.861372</td>\n",
       "      <td>1.856807</td>\n",
       "      <td>Topic 3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>Nuremberg</td>\n",
       "      <td>He was sent to the Cape of Good Hope with lett...</td>\n",
       "      <td>15638655.0</td>\n",
       "      <td>Peter Kolbe</td>\n",
       "      <td>[letter, introduction, mayor, mandate, descrip...</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.440982</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>...</td>\n",
       "      <td>topic_5</td>\n",
       "      <td>0.460599</td>\n",
       "      <td>0.398827</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>6.762273</td>\n",
       "      <td>0.375313</td>\n",
       "      <td>5.861372</td>\n",
       "      <td>1.856807</td>\n",
       "      <td>Topic 3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Nuremberg</td>\n",
       "      <td>Rome</td>\n",
       "      <td>He was sent to the Cape of Good Hope with lett...</td>\n",
       "      <td>15638655.0</td>\n",
       "      <td>Peter Kolbe</td>\n",
       "      <td>[letter, introduction, mayor, mandate, descrip...</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.383563</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>...</td>\n",
       "      <td>topic_5</td>\n",
       "      <td>0.613717</td>\n",
       "      <td>0.398827</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>6.762273</td>\n",
       "      <td>0.375313</td>\n",
       "      <td>5.861372</td>\n",
       "      <td>1.856807</td>\n",
       "      <td>Topic 3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          city_1      city_2  \\\n",
       "0         Toulon        Nice   \n",
       "1     Copenhagen        Oslo   \n",
       "2           Oslo  Copenhagen   \n",
       "3      Barcelona  Dusseldorf   \n",
       "4     Dusseldorf   Barcelona   \n",
       "...          ...         ...   \n",
       "4995        Rome   Amsterdam   \n",
       "4996        Rome   Nuremberg   \n",
       "4997   Amsterdam        Rome   \n",
       "4998   Amsterdam   Nuremberg   \n",
       "4999   Nuremberg        Rome   \n",
       "\n",
       "                                              paragraph  article_id  \\\n",
       "0     Honey from Provence is protected by a red labe...  15415861.0   \n",
       "1     Glosimodt was born in Oslo, Norway. He studied...  15416040.0   \n",
       "2     Glosimodt was born in Oslo, Norway. He studied...  15416040.0   \n",
       "3     On the 24 March 2015, Germanwings Flight 9525 ...  15416113.0   \n",
       "4     On the 24 March 2015, Germanwings Flight 9525 ...  15416113.0   \n",
       "...                                                 ...         ...   \n",
       "4995  He was sent to the Cape of Good Hope with lett...  15638655.0   \n",
       "4996  He was sent to the Cape of Good Hope with lett...  15638655.0   \n",
       "4997  He was sent to the Cape of Good Hope with lett...  15638655.0   \n",
       "4998  He was sent to the Cape of Good Hope with lett...  15638655.0   \n",
       "4999  He was sent to the Cape of Good Hope with lett...  15638655.0   \n",
       "\n",
       "                                title  \\\n",
       "0     Authon, Alpes-de-Haute-Provence   \n",
       "1                      Erik Glosimodt   \n",
       "2                      Erik Glosimodt   \n",
       "3                       Meolans-Revel   \n",
       "4                       Meolans-Revel   \n",
       "...                               ...   \n",
       "4995                      Peter Kolbe   \n",
       "4996                      Peter Kolbe   \n",
       "4997                      Peter Kolbe   \n",
       "4998                      Peter Kolbe   \n",
       "4999                      Peter Kolbe   \n",
       "\n",
       "                                             merged_POS   topic_1   topic_2  \\\n",
       "0     [honey, label, indication, honey, flower, hone...  0.002901  0.072049   \n",
       "1     [education, professor, architect, practice, tr...  0.002901  0.001859   \n",
       "2     [education, professor, architect, practice, tr...  0.018396  0.901826   \n",
       "3                                       [travel, crash]  0.018396  0.011786   \n",
       "4                                       [travel, crash]  0.005012  0.973250   \n",
       "...                                                 ...       ...       ...   \n",
       "4995  [letter, introduction, mayor, mandate, descrip...  0.000791  0.000507   \n",
       "4996  [letter, introduction, mayor, mandate, descrip...  0.000791  0.000507   \n",
       "4997  [letter, introduction, mayor, mandate, descrip...  0.000791  0.000507   \n",
       "4998  [letter, introduction, mayor, mandate, descrip...  0.000791  0.000507   \n",
       "4999  [letter, introduction, mayor, mandate, descrip...  0.000791  0.000507   \n",
       "\n",
       "       topic_3   topic_4  ...  lda_dominant  lda_dominant_score   Topic 1  \\\n",
       "0     0.002811  0.002407  ...       topic_5            0.917025  1.177238   \n",
       "1     0.002811  0.002407  ...       topic_5            0.987215  0.000000   \n",
       "2     0.017823  0.015259  ...       topic_2            0.901826  0.000000   \n",
       "3     0.017823  0.905299  ...       topic_4            0.905299  0.000000   \n",
       "4     0.004856  0.004158  ...       topic_2            0.973250  0.000000   \n",
       "...        ...       ...  ...           ...                 ...       ...   \n",
       "4995  0.421843  0.000656  ...       topic_5            0.575438  0.398827   \n",
       "4996  0.192165  0.000656  ...       topic_5            0.805115  0.398827   \n",
       "4997  0.268724  0.000656  ...       topic_5            0.728556  0.398827   \n",
       "4998  0.440982  0.000656  ...       topic_5            0.460599  0.398827   \n",
       "4999  0.383563  0.000656  ...       topic_5            0.613717  0.398827   \n",
       "\n",
       "       Topic 2   Topic 3   Topic 4   Topic 5   Topic 6  embedding_dominant  \\\n",
       "0     3.370153  7.421871  1.268505  1.032050  1.251641             Topic 3   \n",
       "1     1.535294  1.509655  0.000000  3.934449  0.763062             Topic 5   \n",
       "2     1.535294  1.509655  0.000000  3.934449  0.763062             Topic 5   \n",
       "3     0.578503  0.000000  0.000000  0.000000  0.417536             Topic 2   \n",
       "4     0.578503  0.000000  0.000000  0.000000  0.417536             Topic 2   \n",
       "...        ...       ...       ...       ...       ...                 ...   \n",
       "4995  0.411773  6.762273  0.375313  5.861372  1.856807             Topic 3   \n",
       "4996  0.411773  6.762273  0.375313  5.861372  1.856807             Topic 3   \n",
       "4997  0.411773  6.762273  0.375313  5.861372  1.856807             Topic 3   \n",
       "4998  0.411773  6.762273  0.375313  5.861372  1.856807             Topic 3   \n",
       "4999  0.411773  6.762273  0.375313  5.861372  1.856807             Topic 3   \n",
       "\n",
       "      same_categorisation  \n",
       "0                   False  \n",
       "1                    True  \n",
       "2                   False  \n",
       "3                   False  \n",
       "4                    True  \n",
       "...                   ...  \n",
       "4995                False  \n",
       "4996                False  \n",
       "4997                False  \n",
       "4998                False  \n",
       "4999                False  \n",
       "\n",
       "[5000 rows x 22 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "65616871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_df.to_csv('..\\..\\..\\..\\..\\data\\clean\\lda_classified_30cities_435citypairs_311k_paragraphs_both_methods_with_education.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c8c90d",
   "metadata": {},
   "source": [
    "### 7. Informatics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287c9e4",
   "metadata": {},
   "source": [
    "#### Similarity between lda topic model and word embedding algorithm output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "6b594eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3092 out of 5000 were classified the same by the LDA topic model and word embedding classification model.\n"
     ]
    }
   ],
   "source": [
    "same_categorisation = updated_df[updated_df['same_categorisation'] == True]['same_categorisation'].count()\n",
    "total_documents = updated_df['same_categorisation'].count()\n",
    "\n",
    "print(f\"{same_categorisation} out of {total_documents} were classified the same by the LDA topic model and word embedding classification model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6c41b392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda_dominant  same_categorisation\n",
      "topic_1       True                   376\n",
      "              False                  168\n",
      "topic_2       True                   240\n",
      "              False                  111\n",
      "topic_3       True                   178\n",
      "              False                   51\n",
      "topic_4       True                   531\n",
      "              False                   92\n",
      "topic_5       True                   481\n",
      "              False                  187\n",
      "topic_6       False                  202\n",
      "              True                   156\n",
      "Name: same_categorisation, dtype: int64\n",
      "----------------------------------------------------\n",
      "lda_dominant  same_categorisation\n",
      "topic_1       True                   0.691176\n",
      "              False                  0.308824\n",
      "topic_2       True                   0.683761\n",
      "              False                  0.316239\n",
      "topic_3       True                   0.777293\n",
      "              False                  0.222707\n",
      "topic_4       True                   0.852327\n",
      "              False                  0.147673\n",
      "topic_5       True                   0.720060\n",
      "              False                  0.279940\n",
      "topic_6       False                  0.564246\n",
      "              True                   0.435754\n",
      "Name: same_categorisation, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(updated_df[(updated_df['lda_dominant_score'] > 0.9) & (updated_df['merged_POS'].str.len() > 5)].groupby('lda_dominant')['same_categorisation'].value_counts())\n",
    "print('----------------------------------------------------')\n",
    "print(updated_df[(updated_df['lda_dominant_score'] > 0.9) & (updated_df['merged_POS'].str.len() > 5)].groupby('lda_dominant')['same_categorisation'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97291575",
   "metadata": {},
   "source": [
    "#### binned LDA scores of all & differently classified paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "95bf1697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9, 1.0]       11873\n",
       "(0.8, 0.9]        1721\n",
       "(0.7, 0.8]        1371\n",
       "(0.6, 0.7]        1598\n",
       "(0.5, 0.6]        1354\n",
       "(0.4, 0.5]        2009\n",
       "(0.3, 0.4]          73\n",
       "(0.2, 0.3]           1\n",
       "(0.1, 0.2]           0\n",
       "(-0.001, 0.1]        0\n",
       "Name: lda_dominant_score, dtype: int64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df['lda_dominant_score'].value_counts(bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]).sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5a1ddc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9, 1.0]       3054\n",
       "(0.8, 0.9]        565\n",
       "(0.7, 0.8]        555\n",
       "(0.6, 0.7]        793\n",
       "(0.5, 0.6]        755\n",
       "(0.4, 0.5]       1393\n",
       "(0.3, 0.4]         42\n",
       "(0.2, 0.3]          1\n",
       "(0.1, 0.2]          0\n",
       "(-0.001, 0.1]       0\n",
       "Name: lda_dominant_score, dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df[updated_df['same_categorisation'] != True]['lda_dominant_score'].value_counts(bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]).sort_index(ascending=False) #  .sum() # .sort_index(ascending=False) #.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71df061",
   "metadata": {},
   "source": [
    "#### Percentage of similarly classified documents between LDA topic modeling and word embedding classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "802895b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [146]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m threshold \u001b[38;5;129;01min\u001b[39;00m thresholds:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#x.lda_dominant.endswith(x.embedding_dominant)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(\u001b[43mupdated_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdated_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlda_dominant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdated_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlda_dominant_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdated_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmerged_POS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame_categorisation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#df[(df['idxmax'] == )  & (df['max'] > 0.8)]['outcome'].value_counts(normalize=True)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m topic_values\u001b[38;5;241m.\u001b[39mappend(values)\n",
      "File \u001b[1;32m~\\Personal Files [Local]\\Applied Data Science\\Thesis - CITYNET\\venv_citynet3\\lib\\site-packages\\pandas\\core\\series.py:955\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    952\u001b[0m     key \u001b[38;5;241m=\u001b[39m unpack_1tuple(key)\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_should_fallback_to_positional:\n\u001b[1;32m--> 955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "topics = sorted(['sport', 'art', 'diplomacy', 'education', 'entertainment', 'transportation'])\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "topic_values = []\n",
    "for topic in topics:\n",
    "    values = []\n",
    "    for threshold in thresholds:\n",
    "        #x.lda_dominant.endswith(x.embedding_dominant)\n",
    "        values.append(updated_df[(updated_df['lda_dominant'].str.endswith(topic)) & (updated_df['lda_dominant_score'] > threshold) & (updated_df['merged_POS'].str.len() > 10)]['same_categorisation'].value_counts(normalize=True)[1])\n",
    "        #df[(df['idxmax'] == )  & (df['max'] > 0.8)]['outcome'].value_counts(normalize=True)\n",
    "    topic_values.append(values)\n",
    "\n",
    "topic_values\n",
    "new_topic_values = [[] for x in topic_values[0]]\n",
    "for index, topic in enumerate(topic_values):\n",
    "    for i, value in enumerate(topic):\n",
    "        new_topic_values[i].append(value)\n",
    "print(new_topic_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59520b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 10), facecolor=\"w\")\n",
    "# set width of bars\n",
    "barWidth = 0.25\n",
    "\n",
    "r1 =  np.arange(0, len(topic_values[0])+3, 1.50)\n",
    "# r1 = np.arange(0, len(topic_values[0])+1.5, 1.50)\n",
    "colors =[\"#54bebe\", \"#76c8c8\", \"#98d1d1\", \"#badbdb\", \"#97d2fb\", \"#eccbd9\", \"#eccbd9\"]\n",
    "colors = [ \"#badbdb\",\"#98d1d1\", \"#76c8c8\",\"#54bebe\", \"#63a4da\", \"#296ead\", \"#2a4693\"]\n",
    "\n",
    "for idx, topic in enumerate(new_topic_values):\n",
    "    plt.bar(r1, topic, color=colors[idx], width=barWidth, edgecolor='white', label=thresholds[idx], zorder=3)\n",
    "    r1= [x + barWidth for x in r1]\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.ylabel('Percentage (%)', fontweight='normal')\n",
    "plt.xlabel('Topic')\n",
    "plt.xticks([0.5, 2, 3.5,  5, 6.5, 8], ['Art', 'Diplomacy', 'Education', 'Entertainment', 'Sport', 'Transportation'])\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(-0.2, 9)\n",
    "\n",
    "# Create legend & Show graphic\n",
    "plt.title('Percentage of similarly classified documents by LDA topic modeling and word embeddings')\n",
    "plt.legend(title=\"threshold value\", loc=\"lower right\")\n",
    "plt.grid(zorder=0, color='lightgray', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df62ff",
   "metadata": {},
   "source": [
    "### 8. Aggregate paragraphs Classification into City Pair Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae626faa",
   "metadata": {},
   "source": [
    "#### 8.1 Select right parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_lda_threshold = 0.7\n",
    "minimal_paragraph_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires city1/city2 update!\n",
    "temp_df = updated_df[(updated_df['city_pair'].isin(updated_df['city_pair'].unique()[:])) & (updated_df['lda_dominant_score'] > bottom_lda_threshold) & (updated_df['merged_POS'].str.len() > minimal_paragraph_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa37ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91252aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = temp_df.groupby('city_pair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list2 = [['city_pair', 'paragraphs', 'lemmatised_paragraph_length', 'same_categorisation_raw', 'same_categorisation_percentage',\n",
    "                'lda_dominant_category', 'embedding_dominant_category', 'lda_art', 'embedding_art', 'lda_diplomacy',\n",
    "                'embedding_diplomacy', 'lda_education', 'embedding_education', 'lda_entertainment', 'embedding_entertainment', \n",
    "                'lda_sport', 'embedding_sport', 'lda_transportation', 'embedding_transportation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bcc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ca3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(temp_df['embedding_dominant'].unique()))\n",
    "print(list(temp_df['lda_dominant'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1aba73",
   "metadata": {},
   "source": [
    "#### 8.2 Aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3084fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "lda_categories = list(temp_df['lda_dominant'].unique())\n",
    "embedding_categories = list(temp_df['embedding_dominant'].unique())\n",
    "\n",
    "for city_pair, sub_df in tqdm(grouped_df):\n",
    "    paragraph_count = sub_df['paragraph'].count()\n",
    "    same_categorisation_raw = sub_df['same_categorisation'].sum()\n",
    "    same_categorisation_percentage = sub_df['same_categorisation'].sum()/sub_df['same_categorisation'].count()\n",
    "    lemmatised_paragraph_len = sub_df['merged_POS'].str.len().mean()\n",
    "    \n",
    "    lda_prediction = sub_df['lda_dominant'].value_counts()\n",
    "    embedding_prediction = sub_df['embedding_dominant'].value_counts()\n",
    "        \n",
    "    lda_dominant_category = lda_prediction.idxmax()\n",
    "    embedding_dominant_category = embedding_prediction.idxmax()\n",
    "    \n",
    "    lda_prediction = lda_prediction.to_dict()\n",
    "    embedding_prediction = embedding_prediction.to_dict()\n",
    "    \n",
    "    if (len(lda_prediction) != len(lda_categories)):\n",
    "        for category in lda_categories:\n",
    "            if category not in lda_prediction.keys():\n",
    "                lda_prediction[category] = 0\n",
    "                \n",
    "    if (len(embedding_prediction) != len(embedding_categories)):\n",
    "        for category in embedding_categories:\n",
    "            if category not in embedding_prediction.keys():\n",
    "                embedding_prediction[category] = 0\n",
    "                \n",
    "    temp_l2 = [city_pair, paragraph_count, lemmatised_paragraph_len, same_categorisation_raw, same_categorisation_percentage,\n",
    "                lda_dominant_category, embedding_dominant_category, lda_prediction['lda_art'], embedding_prediction['art'], lda_prediction['lda_diplomacy'],\n",
    "                embedding_prediction['diplomacy'], lda_prediction['lda_education'], embedding_prediction['education'],\n",
    "                lda_prediction['lda_entertainment'], embedding_prediction['entertainment'], lda_prediction['lda_sport'],\n",
    "                embedding_prediction['sport'], lda_prediction['lda_transportation'], embedding_prediction['transportation']]\n",
    "    if len(nested_list2[0]) != len(temp_l2):\n",
    "        raise Exception('Not the same size!')\n",
    "    \n",
    "    nested_list2.append(temp_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6a262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(nested_list2[1:],columns=nested_list2[0])\n",
    "\n",
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa9d0ef",
   "metadata": {},
   "source": [
    "## 8.4 Save Aggregated Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e72a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.to_csv('..\\..\\..\\..\\..\\data\\clean\\deliverable_435city_pairs_both_methods_with_education_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bba1b",
   "metadata": {},
   "source": [
    "#### 8.5 Normalise classification (by number of paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce52a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize category outcomes\n",
    "final_df_normalised = final_df\n",
    "final_df_normalised[list(final_df_normalised.columns)[7:]] = final_df_normalised[list(final_df_normalised.columns)[7:]].div(final_df_normalised['paragraphs'], axis=0) # .count()\n",
    "\n",
    "final_df_normalised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd13ea95",
   "metadata": {},
   "source": [
    "## 8.6 Save Normalised Aggregated Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd22076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df_normalised.to_csv('..\\..\\..\\..\\..\\data\\clean\\deliverable_435city_pairs_both_methods_with_education_final_normalised.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174a210",
   "metadata": {},
   "source": [
    "# EXTRAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23815b5",
   "metadata": {},
   "source": [
    "#### Show closest words to topic vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from preprocessing_functions import *\n",
    "\n",
    "def find_closest_embeddings(embedding, cutoff=25):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda token: spatial.distance.euclidean(embeddings_dict[token], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca79a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'art'\n",
    "\n",
    "words = find_closest_embeddings(embedding=\n",
    "     mean_vectors_dict[topic]    # embeddings_dict['diplomacy'] # embeddings_dict['fashion']\n",
    ")[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = get_english_words(path='../../../input/english_words_alpha_370k.txt\n",
    "print(remove_non_existing_words_from_wordlist(words, english_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
