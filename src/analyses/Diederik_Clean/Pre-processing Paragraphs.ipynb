{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab4bc67",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; border-radius: 5px; padding: 10px;\">\n",
    "    <h4>Pre-processing Paragraphs</h4>\n",
    "    <p>...</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8aa869",
   "metadata": {},
   "source": [
    "## Packages and Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96a5d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Import Scripts\n",
    "from preprocessing_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e9900",
   "metadata": {},
   "source": [
    "## Pre-processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4be34908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise_city_pair(df, POS, OVERWRITE=False, ONLY_ENGLISH_WORDS=False, ENGLISH_WORDS = [],\n",
    "    english_words_file=\"../../../input/english_words_alpha_370k.txt\", NLP_MAX_LENGTH=1500000):\n",
    "    \n",
    "    for tag in tqdm(POS, desc=f\"POS: {POS}\", leave=False):\n",
    "        if OVERWRITE or tag not in df.columns:\n",
    "            df.loc[:, f\"{tag}\"] = lemmatise_paragraphs(paragraphs=df.loc[:, 'paragraph'], POStag=tag, NLP_MAX_LENGTH=NLP_MAX_LENGTH)\n",
    "\n",
    "        if ONLY_ENGLISH_WORDS and (OVERWRITE or f'{tag}_clean' not in df.columns):\n",
    "            df.loc[:, f'{tag}_clean'] = keep_english_words_in_paragraphs(paragraphs=df.loc[:, tag], english_words=ENGLISH_WORDS)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9588b",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0829e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../../data_clean/' # directory where selected articles will be saved, change if you want to save these elsewhere\n",
    "out_dir = 'output/'\n",
    "in_dir = '../../input/'\n",
    "# extr_dir = path/to/wikidump/ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32408cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS = [\"NOUN\", \"VERB\", \"ADJ\"]\n",
    "OVERWRITE=False\n",
    "ONLY_ENGLISH_WORDS=True\n",
    "NLP_MAX_LENGTH = 1500000\n",
    "\n",
    "ENGLISH_WORDS = get_english_words(path=f\"{in_dir}english_words_alpha_370k.txt\")\n",
    "# df =  pd.read_csv(\"../../../../../data/clean/paragraphs/paragraphs_raw_folder_62.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae02098",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20a40ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aded66317d65468090c9cc2c984d0ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pre-processing paragraphs...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraphs_0_101735\n",
      "paragraphs_0_101735_test\n",
      "paragraphs_101735_203469\n",
      "paragraphs_203469_305203\n",
      "paragraphs_305203_406937\n",
      "paragraphs_406937_508671\n"
     ]
    }
   ],
   "source": [
    "paragraphs_dir = os.path.join(data_dir, 'paragraphs')\n",
    "\n",
    "for file in tqdm(os.listdir(paragraphs_dir), desc= 'Pre-processing paragraphs...'):\n",
    "    fp = os.path.join(paragraphs_dir, file)\n",
    "    filename = file.split('.')[0]\n",
    "    \n",
    "    df = pd.read_csv(fp)\n",
    "\n",
    "    lemmatised_df = lemmatise_city_pair(df=df, POS=POS, OVERWRITE=OVERWRITE, ONLY_ENGLISH_WORDS=ONLY_ENGLISH_WORDS, ENGLISH_WORDS=ENGLISH_WORDS, NLP_MAX_LENGTH=NLP_MAX_LENGTH)\n",
    "    lemmatised_df.to_csv(f\"{filename}_preprocessed.csv\", index=False)\n",
    "    \n",
    "    print(f\"Pre-processed {filename}, containing {len(df)} paragraphs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae4f1df",
   "metadata": {},
   "source": [
    "## Everything from here may be deleted later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e1a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = os.path.join(paragraphs_dir, \"paragraphs_0_101735_test.csv\")\n",
    "df =  pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd1bee4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_1</th>\n",
       "      <th>city_2</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Birmingham</td>\n",
       "      <td>Florence</td>\n",
       "      <td>1</td>\n",
       "      <td>The first community of adherents of the Baha'i...</td>\n",
       "      <td>303</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Florence</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>2</td>\n",
       "      <td>The first community of adherents of the Baha'i...</td>\n",
       "      <td>303</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paris</td>\n",
       "      <td>London</td>\n",
       "      <td>3</td>\n",
       "      <td>A major revision of the work by composer and a...</td>\n",
       "      <td>309</td>\n",
       "      <td>An American in Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>London</td>\n",
       "      <td>Paris</td>\n",
       "      <td>4</td>\n",
       "      <td>A major revision of the work by composer and a...</td>\n",
       "      <td>309</td>\n",
       "      <td>An American in Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Madrid</td>\n",
       "      <td>Rome</td>\n",
       "      <td>5</td>\n",
       "      <td>Access to biocapacity in Algeria is lower than...</td>\n",
       "      <td>358</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488331</th>\n",
       "      <td>Dublin</td>\n",
       "      <td>London</td>\n",
       "      <td>488332</td>\n",
       "      <td>Allman-Smith played hockey for Dublin Universi...</td>\n",
       "      <td>3001932</td>\n",
       "      <td>Edward Allman-Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488332</th>\n",
       "      <td>London</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>488333</td>\n",
       "      <td>O'Kelly and Condell met in Dublin in 1969 and ...</td>\n",
       "      <td>3001953</td>\n",
       "      <td>Tir na nOg (band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488333</th>\n",
       "      <td>Dublin</td>\n",
       "      <td>London</td>\n",
       "      <td>488334</td>\n",
       "      <td>O'Kelly and Condell met in Dublin in 1969 and ...</td>\n",
       "      <td>3001953</td>\n",
       "      <td>Tir na nOg (band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488334</th>\n",
       "      <td>Birmingham</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>488335</td>\n",
       "      <td>Tir na nOg reformed in 1985, releasing the sin...</td>\n",
       "      <td>3001953</td>\n",
       "      <td>Tir na nOg (band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488335</th>\n",
       "      <td>Dublin</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>488336</td>\n",
       "      <td>Tir na nOg reformed in 1985, releasing the sin...</td>\n",
       "      <td>3001953</td>\n",
       "      <td>Tir na nOg (band)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>488336 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            city_1      city_2  paragraph_id  \\\n",
       "0       Birmingham    Florence             1   \n",
       "1         Florence  Birmingham             2   \n",
       "2            Paris      London             3   \n",
       "3           London       Paris             4   \n",
       "4           Madrid        Rome             5   \n",
       "...            ...         ...           ...   \n",
       "488331      Dublin      London        488332   \n",
       "488332      London      Dublin        488333   \n",
       "488333      Dublin      London        488334   \n",
       "488334  Birmingham      Dublin        488335   \n",
       "488335      Dublin  Birmingham        488336   \n",
       "\n",
       "                                                paragraph  article_id  \\\n",
       "0       The first community of adherents of the Baha'i...         303   \n",
       "1       The first community of adherents of the Baha'i...         303   \n",
       "2       A major revision of the work by composer and a...         309   \n",
       "3       A major revision of the work by composer and a...         309   \n",
       "4       Access to biocapacity in Algeria is lower than...         358   \n",
       "...                                                   ...         ...   \n",
       "488331  Allman-Smith played hockey for Dublin Universi...     3001932   \n",
       "488332  O'Kelly and Condell met in Dublin in 1969 and ...     3001953   \n",
       "488333  O'Kelly and Condell met in Dublin in 1969 and ...     3001953   \n",
       "488334  Tir na nOg reformed in 1985, releasing the sin...     3001953   \n",
       "488335  Tir na nOg reformed in 1985, releasing the sin...     3001953   \n",
       "\n",
       "                       title  \n",
       "0                    Alabama  \n",
       "1                    Alabama  \n",
       "2       An American in Paris  \n",
       "3       An American in Paris  \n",
       "4                    Algeria  \n",
       "...                      ...  \n",
       "488331   Edward Allman-Smith  \n",
       "488332     Tir na nOg (band)  \n",
       "488333     Tir na nOg (band)  \n",
       "488334     Tir na nOg (band)  \n",
       "488335     Tir na nOg (band)  \n",
       "\n",
       "[488336 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f70c6637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['NOUN', 'VERB', 'ADJ']:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (NOUN)...:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (VERB)...:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (ADJ)...:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7min 57s\n",
      "Wall time: 23min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lemmatised_df = lemmatise_city_pair(df=df[:50000], POS=POS, OVERWRITE=OVERWRITE, ONLY_ENGLISH_WORDS=ONLY_ENGLISH_WORDS, ENGLISH_WORDS=ENGLISH_WORDS, NLP_MAX_LENGTH=NLP_MAX_LENGTH)\n",
    "lemmatised_df.to_csv(\"lemmatised_file.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "993ebbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_1</th>\n",
       "      <th>city_2</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NOUN_clean</th>\n",
       "      <th>VERB</th>\n",
       "      <th>VERB_clean</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADJ_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Birmingham</td>\n",
       "      <td>Florence</td>\n",
       "      <td>1</td>\n",
       "      <td>The first community of adherents of the Baha'i...</td>\n",
       "      <td>303</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>['community', 'adherent', 'center']</td>\n",
       "      <td>['community', 'adherent', 'center']</td>\n",
       "      <td>['found', 'move', 'exist']</td>\n",
       "      <td>['found', 'move', 'exist']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Florence</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>2</td>\n",
       "      <td>The first community of adherents of the Baha'i...</td>\n",
       "      <td>303</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>['community', 'adherent', 'center']</td>\n",
       "      <td>['community', 'adherent', 'center']</td>\n",
       "      <td>['found', 'move', 'exist']</td>\n",
       "      <td>['found', 'move', 'exist']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paris</td>\n",
       "      <td>London</td>\n",
       "      <td>3</td>\n",
       "      <td>A major revision of the work by composer and a...</td>\n",
       "      <td>309</td>\n",
       "      <td>An American in Paris</td>\n",
       "      <td>['revision', 'work', 'composer', 'arranger', '...</td>\n",
       "      <td>['revision', 'work', 'composer', 'arranger', '...</td>\n",
       "      <td>['simplify', 'reduce', 'eliminate', 'avoid', '...</td>\n",
       "      <td>['simplify', 'reduce', 'eliminate', 'avoid', '...</td>\n",
       "      <td>['major', 'standard', 'original', 'original']</td>\n",
       "      <td>['major', 'standard', 'original', 'original']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>London</td>\n",
       "      <td>Paris</td>\n",
       "      <td>4</td>\n",
       "      <td>A major revision of the work by composer and a...</td>\n",
       "      <td>309</td>\n",
       "      <td>An American in Paris</td>\n",
       "      <td>['revision', 'work', 'composer', 'arranger', '...</td>\n",
       "      <td>['revision', 'work', 'composer', 'arranger', '...</td>\n",
       "      <td>['simplify', 'reduce', 'eliminate', 'avoid', '...</td>\n",
       "      <td>['simplify', 'reduce', 'eliminate', 'avoid', '...</td>\n",
       "      <td>['major', 'standard', 'original', 'original']</td>\n",
       "      <td>['major', 'standard', 'original', 'original']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Madrid</td>\n",
       "      <td>Rome</td>\n",
       "      <td>5</td>\n",
       "      <td>Access to biocapacity in Algeria is lower than...</td>\n",
       "      <td>358</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>['access', 'biocapacity', 'world', 'hectare', ...</td>\n",
       "      <td>['access', 'world', 'hectare', 'person', 'terr...</td>\n",
       "      <td>['mean', 'use', 'contain', 'run', 'hold', 'sec...</td>\n",
       "      <td>['mean', 'use', 'contain', 'run', 'hold', 'sec...</td>\n",
       "      <td>['low', 'average', 'global', 'global', 'global...</td>\n",
       "      <td>['low', 'average', 'global', 'global', 'global...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Stockholm</td>\n",
       "      <td>996</td>\n",
       "      <td>Athens was awarded the 2004 Summer Olympics on...</td>\n",
       "      <td>1216</td>\n",
       "      <td>Athens</td>\n",
       "      <td>['bid', 'time', 'game', 'event', 'bid', 'bid',...</td>\n",
       "      <td>['bid', 'time', 'game', 'event', 'bid', 'bid',...</td>\n",
       "      <td>['award', 'have', 'lose', 'host', 'host', 'fol...</td>\n",
       "      <td>['award', 'have', 'lose', 'host', 'host', 'fol...</td>\n",
       "      <td>['previous', 'second', 'inaugural', 'unsuccess...</td>\n",
       "      <td>['previous', 'second', 'inaugural', 'unsuccess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Rome</td>\n",
       "      <td>Athens</td>\n",
       "      <td>997</td>\n",
       "      <td>Athens was awarded the 2004 Summer Olympics on...</td>\n",
       "      <td>1216</td>\n",
       "      <td>Athens</td>\n",
       "      <td>['bid', 'time', 'game', 'event', 'bid', 'bid',...</td>\n",
       "      <td>['bid', 'time', 'game', 'event', 'bid', 'bid',...</td>\n",
       "      <td>['award', 'have', 'lose', 'host', 'host', 'fol...</td>\n",
       "      <td>['award', 'have', 'lose', 'host', 'host', 'fol...</td>\n",
       "      <td>['previous', 'second', 'inaugural', 'unsuccess...</td>\n",
       "      <td>['previous', 'second', 'inaugural', 'unsuccess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Rome</td>\n",
       "      <td>Stockholm</td>\n",
       "      <td>998</td>\n",
       "      <td>Athens was awarded the 2004 Summer Olympics on...</td>\n",
       "      <td>1216</td>\n",
       "      <td>Athens</td>\n",
       "      <td>['bid', 'time', 'game', 'event', 'bid', 'bid',...</td>\n",
       "      <td>['bid', 'time', 'game', 'event', 'bid', 'bid',...</td>\n",
       "      <td>['award', 'have', 'lose', 'host', 'host', 'fol...</td>\n",
       "      <td>['award', 'have', 'lose', 'host', 'host', 'fol...</td>\n",
       "      <td>['previous', 'second', 'inaugural', 'unsuccess...</td>\n",
       "      <td>['previous', 'second', 'inaugural', 'unsuccess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Stockholm</td>\n",
       "      <td>Athens</td>\n",
       "      <td>999</td>\n",
       "      <td>Athens was awarded the 2004 Summer Olympics on...</td>\n",
       "      <td>1216</td>\n",
       "      <td>Athens</td>\n",
       "      <td>['bid', 'time', 'game', 'event', 'bid', 'bid',...</td>\n",
       "      <td>['bid', 'time', 'game', 'event', 'bid', 'bid',...</td>\n",
       "      <td>['award', 'have', 'lose', 'host', 'host', 'fol...</td>\n",
       "      <td>['award', 'have', 'lose', 'host', 'host', 'fol...</td>\n",
       "      <td>['previous', 'second', 'inaugural', 'unsuccess...</td>\n",
       "      <td>['previous', 'second', 'inaugural', 'unsuccess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Stockholm</td>\n",
       "      <td>Rome</td>\n",
       "      <td>1000</td>\n",
       "      <td>Athens was awarded the 2004 Summer Olympics on...</td>\n",
       "      <td>1216</td>\n",
       "      <td>Athens</td>\n",
       "      <td>['bid', 'time', 'game', 'event', 'bid', 'bid',...</td>\n",
       "      <td>['bid', 'time', 'game', 'event', 'bid', 'bid',...</td>\n",
       "      <td>['award', 'have', 'lose', 'host', 'host', 'fol...</td>\n",
       "      <td>['award', 'have', 'lose', 'host', 'host', 'fol...</td>\n",
       "      <td>['previous', 'second', 'inaugural', 'unsuccess...</td>\n",
       "      <td>['previous', 'second', 'inaugural', 'unsuccess...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         city_1      city_2  paragraph_id  \\\n",
       "0    Birmingham    Florence             1   \n",
       "1      Florence  Birmingham             2   \n",
       "2         Paris      London             3   \n",
       "3        London       Paris             4   \n",
       "4        Madrid        Rome             5   \n",
       "..          ...         ...           ...   \n",
       "995      Athens   Stockholm           996   \n",
       "996        Rome      Athens           997   \n",
       "997        Rome   Stockholm           998   \n",
       "998   Stockholm      Athens           999   \n",
       "999   Stockholm        Rome          1000   \n",
       "\n",
       "                                             paragraph  article_id  \\\n",
       "0    The first community of adherents of the Baha'i...         303   \n",
       "1    The first community of adherents of the Baha'i...         303   \n",
       "2    A major revision of the work by composer and a...         309   \n",
       "3    A major revision of the work by composer and a...         309   \n",
       "4    Access to biocapacity in Algeria is lower than...         358   \n",
       "..                                                 ...         ...   \n",
       "995  Athens was awarded the 2004 Summer Olympics on...        1216   \n",
       "996  Athens was awarded the 2004 Summer Olympics on...        1216   \n",
       "997  Athens was awarded the 2004 Summer Olympics on...        1216   \n",
       "998  Athens was awarded the 2004 Summer Olympics on...        1216   \n",
       "999  Athens was awarded the 2004 Summer Olympics on...        1216   \n",
       "\n",
       "                    title                                               NOUN  \\\n",
       "0                 Alabama                ['community', 'adherent', 'center']   \n",
       "1                 Alabama                ['community', 'adherent', 'center']   \n",
       "2    An American in Paris  ['revision', 'work', 'composer', 'arranger', '...   \n",
       "3    An American in Paris  ['revision', 'work', 'composer', 'arranger', '...   \n",
       "4                 Algeria  ['access', 'biocapacity', 'world', 'hectare', ...   \n",
       "..                    ...                                                ...   \n",
       "995                Athens  ['bid', 'time', 'game', 'event', 'bid', 'bid',...   \n",
       "996                Athens  ['bid', 'time', 'game', 'event', 'bid', 'bid',...   \n",
       "997                Athens  ['bid', 'time', 'game', 'event', 'bid', 'bid',...   \n",
       "998                Athens  ['bid', 'time', 'game', 'event', 'bid', 'bid',...   \n",
       "999                Athens  ['bid', 'time', 'game', 'event', 'bid', 'bid',...   \n",
       "\n",
       "                                            NOUN_clean  \\\n",
       "0                  ['community', 'adherent', 'center']   \n",
       "1                  ['community', 'adherent', 'center']   \n",
       "2    ['revision', 'work', 'composer', 'arranger', '...   \n",
       "3    ['revision', 'work', 'composer', 'arranger', '...   \n",
       "4    ['access', 'world', 'hectare', 'person', 'terr...   \n",
       "..                                                 ...   \n",
       "995  ['bid', 'time', 'game', 'event', 'bid', 'bid',...   \n",
       "996  ['bid', 'time', 'game', 'event', 'bid', 'bid',...   \n",
       "997  ['bid', 'time', 'game', 'event', 'bid', 'bid',...   \n",
       "998  ['bid', 'time', 'game', 'event', 'bid', 'bid',...   \n",
       "999  ['bid', 'time', 'game', 'event', 'bid', 'bid',...   \n",
       "\n",
       "                                                  VERB  \\\n",
       "0                           ['found', 'move', 'exist']   \n",
       "1                           ['found', 'move', 'exist']   \n",
       "2    ['simplify', 'reduce', 'eliminate', 'avoid', '...   \n",
       "3    ['simplify', 'reduce', 'eliminate', 'avoid', '...   \n",
       "4    ['mean', 'use', 'contain', 'run', 'hold', 'sec...   \n",
       "..                                                 ...   \n",
       "995  ['award', 'have', 'lose', 'host', 'host', 'fol...   \n",
       "996  ['award', 'have', 'lose', 'host', 'host', 'fol...   \n",
       "997  ['award', 'have', 'lose', 'host', 'host', 'fol...   \n",
       "998  ['award', 'have', 'lose', 'host', 'host', 'fol...   \n",
       "999  ['award', 'have', 'lose', 'host', 'host', 'fol...   \n",
       "\n",
       "                                            VERB_clean  \\\n",
       "0                           ['found', 'move', 'exist']   \n",
       "1                           ['found', 'move', 'exist']   \n",
       "2    ['simplify', 'reduce', 'eliminate', 'avoid', '...   \n",
       "3    ['simplify', 'reduce', 'eliminate', 'avoid', '...   \n",
       "4    ['mean', 'use', 'contain', 'run', 'hold', 'sec...   \n",
       "..                                                 ...   \n",
       "995  ['award', 'have', 'lose', 'host', 'host', 'fol...   \n",
       "996  ['award', 'have', 'lose', 'host', 'host', 'fol...   \n",
       "997  ['award', 'have', 'lose', 'host', 'host', 'fol...   \n",
       "998  ['award', 'have', 'lose', 'host', 'host', 'fol...   \n",
       "999  ['award', 'have', 'lose', 'host', 'host', 'fol...   \n",
       "\n",
       "                                                   ADJ  \\\n",
       "0                                                   []   \n",
       "1                                                   []   \n",
       "2        ['major', 'standard', 'original', 'original']   \n",
       "3        ['major', 'standard', 'original', 'original']   \n",
       "4    ['low', 'average', 'global', 'global', 'global...   \n",
       "..                                                 ...   \n",
       "995  ['previous', 'second', 'inaugural', 'unsuccess...   \n",
       "996  ['previous', 'second', 'inaugural', 'unsuccess...   \n",
       "997  ['previous', 'second', 'inaugural', 'unsuccess...   \n",
       "998  ['previous', 'second', 'inaugural', 'unsuccess...   \n",
       "999  ['previous', 'second', 'inaugural', 'unsuccess...   \n",
       "\n",
       "                                             ADJ_clean  \n",
       "0                                                   []  \n",
       "1                                                   []  \n",
       "2        ['major', 'standard', 'original', 'original']  \n",
       "3        ['major', 'standard', 'original', 'original']  \n",
       "4    ['low', 'average', 'global', 'global', 'global...  \n",
       "..                                                 ...  \n",
       "995  ['previous', 'second', 'inaugural', 'unsuccess...  \n",
       "996  ['previous', 'second', 'inaugural', 'unsuccess...  \n",
       "997  ['previous', 'second', 'inaugural', 'unsuccess...  \n",
       "998  ['previous', 'second', 'inaugural', 'unsuccess...  \n",
       "999  ['previous', 'second', 'inaugural', 'unsuccess...  \n",
       "\n",
       "[1000 rows x 12 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('lemmatised_file.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c634d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
